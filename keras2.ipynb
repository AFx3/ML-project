{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#from keras import Dense, Sequential, SGD, l2\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "#from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from keras import backend as K\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import time\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tr(file_path, test_size=0.15, random_state=42):\n",
    "    # load tr data\n",
    "    train = loadtxt(file_path, delimiter=',', usecols=range(1, 14), dtype=np.float64)\n",
    "\n",
    "    # Esclude la prima colonna e le ultime tre colonne (target)\n",
    "    x = train[:, :-3]\n",
    "    y = train[:, -3:]  # Le ultime tre colonne rappresentano i target\n",
    "\n",
    "    # Suddivide il dataset in set di addestramento e test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def read_ts():\n",
    "    file = \"./cup/ds/ML-CUP23-TS.csv\"\n",
    "    test = loadtxt(file, delimiter=',', usecols=range(1, 11), dtype=np.float64)\n",
    "\n",
    "    return test\n",
    "\n",
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n",
    "'''\n",
    "def create_model(layers=3, n_units=30, init_mode='glorot_normal', activation='tanh', lmb=0.0001, eta=0.002, alpha=0.01, batch_size=None):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # create hidden layers\n",
    "    for i in range(layers):\n",
    "        # dense is for units fully connected \n",
    "        model.add(Dense(n_units, kernel_initializer=init_mode, activation=activation, kernel_regularizer=l1_l2(lmb))) \n",
    "\n",
    "    # create output layer with 3 neurons for x, y, z\n",
    "    model.add(Dense(3, activation='linear', kernel_initializer=init_mode))\n",
    "    \n",
    "    # use SGD optimizer\n",
    "    optimizer = SGD(learning_rate=eta, momentum=alpha)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=euclidean_distance_loss, metrics=[euclidean_distance_loss])  # Aggiunto metrics=[euclidean_distance_loss]\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "Best Parameters: {'alpha': 0.8, 'batch_size': 100, 'eta': 0.004, 'init_mode': 'glorot_normal', 'lmb': 0.005, 'n_units': 25}\n",
    "Best Validation Loss: 2.98101544380188\n",
    "Best Euclidean Distance Score: 1.5518332381772344\n",
    "\n",
    "'''\n",
    "def create_model(layers=3, n_units=200, init_mode='glorot_normal',activation='relu', lmb=0.005, eta=0.004, alpha=0.08, batch_size=None): #relu\n",
    "    model = Sequential()\n",
    "    \n",
    "    # create hidden layers with tanh activation\n",
    "    for i in range(layers - 1):  # Last layer will have ReLU activation\n",
    "        model.add(Dense(n_units, kernel_initializer=init_mode, activation=activation, kernel_regularizer=l2(lmb)))\n",
    "    \n",
    "    # create output layer with linear activation\n",
    "    model.add(Dense(3, kernel_initializer=init_mode, activation='linear'))\n",
    "\n",
    "    # use SGD optimizer\n",
    "    optimizer = SGD(learning_rate=eta, momentum=alpha)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=euclidean_distance_loss, metrics=[euclidean_distance_loss])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "    \n",
    "# it retrieves the mean value of all the passed losses\n",
    "def euclidean_distance_score(y_true, y_pred):\n",
    "    return np.mean(euclidean_distance_loss(y_true, y_pred))\n",
    "\n",
    "\n",
    "scorer = make_scorer(euclidean_distance_score, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "#params = dict(eta=0.004, alpha=0.8, lmb=0.005, epochs=250, batch_size=100)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "primo okk con l2\n",
    "\n",
    "Best Parameters: {'alpha': 0.9, 'batch_size': 100, 'eta': 0.005, 'init_mode': 'glorot_normal', 'lmb': 0.0052, 'n_units': 25}\n",
    "Best Validation Loss: 1.8620363473892212\n",
    "Best Euclidean Distance Score: 1.1829318654716738\n",
    "\n",
    "'''\n",
    "'''\n",
    "good stuff \n",
    "eta=0.004, alpha=0.8, lmb=0.0053, epochs=250, batch_size=120)\n",
    "'''\n",
    "\n",
    "def model_selection(x, y, epochs=200):\n",
    "    param_grid = {\n",
    "        'n_units': [300],\n",
    "        'init_mode': ['glorot_normal'],\n",
    "        #'activation': ['relu'],\n",
    "        #'lmb': [0.0001,0.0005, 0.0008, 0.001]\n",
    "        'lmb':[0.01, 0.1, 0.005, 0.01, 0.02, 0.001, 0.002, 0.003],\n",
    "        'eta': [0.004, 0.005, 0.001],\n",
    "        'alpha': [0.5, 0.8, 0.9],\n",
    "        #'batch_size': [32, 64,100, 128]  # Aggiungi diverse dimensioni del batch\n",
    "        'batch_size': [100,110,120]\n",
    "    }\n",
    "\n",
    "    param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "    best_params = None\n",
    "    best_loss = float('inf')  # Inizializza il miglior punteggio con un valore molto grande\n",
    "\n",
    "    for params in param_combinations:\n",
    "        # Crea e addestra il modello con gli iperparametri correnti\n",
    "        model = create_model(layers=3, **params)\n",
    "        history = model.fit(x, y, validation_split=0.3, epochs=epochs, batch_size=params['batch_size'], verbose=0)\n",
    "\n",
    "        # Calcola la loss finale sul set di validazione\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "        # Calcola l'Euclidean Distance Loss\n",
    "        val_score = euclidean_distance_score(y, model.predict(x))\n",
    "\n",
    "        # Stampa euclidean_distance_loss e validation loss per ogni epoca\n",
    "        print(f\"\\nParameters: {params}\")\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} - Euclidean Distance Loss: {history.history['euclidean_distance_loss'][epoch]:.4f} - Val Loss: {history.history['val_loss'][epoch]:.4f}\")\n",
    "\n",
    "        # Plot learning curve\n",
    "        plot_learning_curve(history.history, epochs=epochs, start_epoch=1, **params)\n",
    "\n",
    "        # Aggiorna il miglior punteggio e i relativi iperparametri se necessario\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_params = params\n",
    "            best_score = val_score\n",
    "\n",
    "    print(\"\\nBest Parameters:\", best_params)\n",
    "    print(\"Best Validation Loss:\", best_loss)\n",
    "    print(\"Best Euclidean Distance Score:\", best_score)\n",
    "\n",
    "    return best_params\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# grid search parallelized\\nfrom joblib import Parallel, delayed\\nfrom sklearn.model_selection import ParameterGrid\\n\\ndef train_and_evaluate_model(params, x, y, epochs=1000):\\n    model = create_model(layers=3, **params)\\n    history = model.fit(x, y, validation_split=0.3, epochs=epochs, batch_size=params[\\'batch_size\\'], verbose=0)\\n    \\n    val_loss = history.history[\\'val_loss\\'][-1]\\n    val_score = euclidean_distance_score(y, model.predict(x))\\n    \\n    print(f\"\\nParameters: {params}\")\\n    for epoch in range(epochs):\\n        print(f\"Epoch {epoch + 1}/{epochs} - Euclidean Distance Loss: {history.history[\\'euclidean_distance_loss\\'][epoch]:.4f} - Val Loss: {history.history[\\'val_loss\\'][epoch]:.4f}\")\\n\\n    plot_learning_curve(history.history, epochs=epochs, start_epoch=1, **params)\\n\\n    return params, val_loss, val_score, history.history\\n\\ndef model_selection(x, y, epochs=1000, n_jobs=-1):\\n    param_grid = {\\n        \\'n_units\\': [20, 30],\\n        \\'init_mode\\': [\\'glorot_normal\\'],\\n        \\'activation\\': [\\'tanh\\'],\\n        \\'lmb\\': [0.0001, 0.0005, 0.0008, 0.001, 0.005, 0.008, 0.01, 0.05, 0.08],\\n        \\'eta\\': [0.001, 0.002, 0.003],\\n        \\'alpha\\': [0.8, 0.9, 0.95],\\n        \\'batch_size\\': [100, 128]\\n    }\\n\\n    param_combinations = list(ParameterGrid(param_grid))\\n\\n    # Define the function to be parallelized\\n    def process_params(params):\\n        return train_and_evaluate_model(params, x, y, epochs)\\n\\n    # Parallelize the grid search\\n    results = Parallel(n_jobs=n_jobs)(delayed(process_params)(params) for params in param_combinations)\\n\\n    best_params, best_loss, best_score, best_history = min(results, key=lambda x: x[1])  # Assuming you\\'re minimizing the loss\\n\\n    print(\"\\nBest Parameters:\", best_params)\\n    print(\"Best Validation Loss:\", best_loss)\\n    print(\"Best Euclidean Distance Score:\", best_score)\\n\\n    return best_params, best_history\\n\\n\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# grid search parallelized\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def train_and_evaluate_model(params, x, y, epochs=1000):\n",
    "    model = create_model(layers=3, **params)\n",
    "    history = model.fit(x, y, validation_split=0.3, epochs=epochs, batch_size=params['batch_size'], verbose=0)\n",
    "    \n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    val_score = euclidean_distance_score(y, model.predict(x))\n",
    "    \n",
    "    print(f\"\\nParameters: {params}\")\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Euclidean Distance Loss: {history.history['euclidean_distance_loss'][epoch]:.4f} - Val Loss: {history.history['val_loss'][epoch]:.4f}\")\n",
    "\n",
    "    plot_learning_curve(history.history, epochs=epochs, start_epoch=1, **params)\n",
    "\n",
    "    return params, val_loss, val_score, history.history\n",
    "\n",
    "def model_selection(x, y, epochs=1000, n_jobs=-1):\n",
    "    param_grid = {\n",
    "        'n_units': [20, 30],\n",
    "        'init_mode': ['glorot_normal'],\n",
    "        'activation': ['tanh'],\n",
    "        'lmb': [0.0001, 0.0005, 0.0008, 0.001, 0.005, 0.008, 0.01, 0.05, 0.08],\n",
    "        'eta': [0.001, 0.002, 0.003],\n",
    "        'alpha': [0.8, 0.9, 0.95],\n",
    "        'batch_size': [100, 128]\n",
    "    }\n",
    "\n",
    "    param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "    # Define the function to be parallelized\n",
    "    def process_params(params):\n",
    "        return train_and_evaluate_model(params, x, y, epochs)\n",
    "\n",
    "    # Parallelize the grid search\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(process_params)(params) for params in param_combinations)\n",
    "\n",
    "    best_params, best_loss, best_score, best_history = min(results, key=lambda x: x[1])  # Assuming you're minimizing the loss\n",
    "\n",
    "    print(\"\\nBest Parameters:\", best_params)\n",
    "    print(\"Best Validation Loss:\", best_loss)\n",
    "    print(\"Best Euclidean Distance Score:\", best_score)\n",
    "\n",
    "    return best_params, best_history\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x_ts, x_its, y_its):\n",
    "    # predict on internal test set\n",
    "    y_ipred = model.predict(x_its)\n",
    "    _, iloss = model.evaluate(x_its, y_its, verbose=0)  # Utilizzo di evaluate invece di calcolare manualmente la loss\n",
    "\n",
    "    # predict on blind test set\n",
    "    y_pred = model.predict(x_ts)\n",
    "\n",
    "    # return predicted target on blind test set,\n",
    "    # and losses on internal test set\n",
    "    return [y_pred[:, i] for i in range(y_pred.shape[1])], iloss\n",
    "#y_pred sarà una matrice in cui ogni colonna rappresenta le previsioni per una delle tre variabili target. \n",
    "# La funzione restituirà quindi una lista di array, uno per ciascuna colonna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(history, start_epoch=1, **kwargs):\n",
    "    lgd = ['Loss TR']\n",
    "    plt.plot(range(start_epoch, kwargs['epochs']), history['loss'][start_epoch:])\n",
    "    \n",
    "    if \"val_loss\" in history:\n",
    "        plt.plot(range(start_epoch, kwargs['epochs']), history['val_loss'][start_epoch:])\n",
    "        lgd.append('Loss VL')\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f'Keras Learning Curve \\n {kwargs}')\n",
    "    plt.legend(lgd)\n",
    "\n",
    "    # Check if predictions are available in the history\n",
    "    if 'predictions' in history:\n",
    "        predictions = history['predictions']\n",
    "        \n",
    "        # Plot predictions for each variable\n",
    "        for i in range(predictions.shape[1]):\n",
    "            plt.figure()\n",
    "            plt.plot(range(start_epoch, kwargs['epochs']), predictions[:, i][start_epoch:])\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(f\"Prediction Variable {i+1}\")\n",
    "            plt.title(f'Keras Learning Curve \\n {kwargs} - Prediction Variable {i+1}')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_nn(ms=False):\n",
    "    print(\"keras start\")\n",
    "\n",
    "    file_path_tr = \"./cup/ds/ML-CUP23-TR.csv\"\n",
    "    # read training set\n",
    "    x, y, x_its, y_its = read_tr(file_path_tr)\n",
    "\n",
    "    # choose model selection or hand-given parameters\n",
    "    if ms:\n",
    "        params = model_selection(x, y)\n",
    "    else:\n",
    "        params = dict(eta=0.00125, alpha=0.9, lmb=0.002, epochs=300, batch_size=110)\n",
    "        #Parameters: {'activation': 'tanh', 'alpha': 0.9, 'batch_size': 128, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.001, 'n_units': 30}\n",
    "        \n",
    "        #Parameters: {'activation': 'tanh', 'alpha': 0.9, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.0001, 'n_units': 30}\n",
    "#Best Parameters: {'activation': 'tanh', 'alpha': 0.9, 'eta': 0.003, 'init_mode': 'glorot_uniform', 'lmb': 0.0001, 'n_units': 30}\n",
    "\n",
    "    # create and fit the model\n",
    "    model = create_model(eta=params['eta'], alpha=params['alpha'], lmb=params['lmb'])\n",
    "    res = model.fit(x, y, validation_split=0.3, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1)\n",
    "\n",
    "    tr_losses = res.history['loss']\n",
    "    val_losses = res.history['val_loss']\n",
    "    \n",
    "    \n",
    "  # Predict for the three variables\n",
    "    y_pred, ts_losses = predict(model=model, x_ts=read_ts(), x_its=x_its, y_its=y_its)\n",
    "\n",
    "    print(\"TR Loss: \", tr_losses[-1])\n",
    "    print(\"VL Loss: \", val_losses[-1])\n",
    "    print(\"TS Loss: \", np.mean(ts_losses))\n",
    "\n",
    "# Extract predictions for each variable\n",
    "    y_pred_x, y_pred_y, y_pred_z = y_pred\n",
    "\n",
    "    print(\"Predictions for X: \", y_pred_x)  \n",
    "    print(\"Predictions for Y: \", y_pred_y)\n",
    "    print(\"Predictions for Z: \", y_pred_z)\n",
    "\n",
    "    print(\"keras end\")\n",
    "\n",
    "    plot_learning_curve(res.history, savefig=True, **params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras start\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 17ms/step - loss: 43.1433 - euclidean_distance_loss: 42.7035 - val_loss: 43.2252 - val_euclidean_distance_loss: 42.7854\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 43.0685 - euclidean_distance_loss: 42.6288 - val_loss: 43.1157 - val_euclidean_distance_loss: 42.6760\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.9487 - euclidean_distance_loss: 42.5091 - val_loss: 42.9731 - val_euclidean_distance_loss: 42.5335\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.8012 - euclidean_distance_loss: 42.3617 - val_loss: 42.8103 - val_euclidean_distance_loss: 42.3709\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.6368 - euclidean_distance_loss: 42.1973 - val_loss: 42.6307 - val_euclidean_distance_loss: 42.1913\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.4572 - euclidean_distance_loss: 42.0177 - val_loss: 42.4330 - val_euclidean_distance_loss: 41.9936\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.2585 - euclidean_distance_loss: 41.8191 - val_loss: 42.2147 - val_euclidean_distance_loss: 41.7752\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 42.0414 - euclidean_distance_loss: 41.6018 - val_loss: 41.9685 - val_euclidean_distance_loss: 41.5288\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.7907 - euclidean_distance_loss: 41.3508 - val_loss: 41.6926 - val_euclidean_distance_loss: 41.2525\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.5110 - euclidean_distance_loss: 41.0708 - val_loss: 41.3730 - val_euclidean_distance_loss: 40.9325\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.1893 - euclidean_distance_loss: 40.7486 - val_loss: 40.9990 - val_euclidean_distance_loss: 40.5579\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 40.8150 - euclidean_distance_loss: 40.3737 - val_loss: 40.5631 - val_euclidean_distance_loss: 40.1213\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 40.3761 - euclidean_distance_loss: 39.9340 - val_loss: 40.0548 - val_euclidean_distance_loss: 39.6121\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39.8693 - euclidean_distance_loss: 39.4262 - val_loss: 39.4694 - val_euclidean_distance_loss: 39.0257\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39.3080 - euclidean_distance_loss: 38.8637 - val_loss: 38.7969 - val_euclidean_distance_loss: 38.3518\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.6708 - euclidean_distance_loss: 38.2252 - val_loss: 38.0748 - val_euclidean_distance_loss: 37.6282\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.9828 - euclidean_distance_loss: 37.5356 - val_loss: 37.3273 - val_euclidean_distance_loss: 36.8791\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 37.2882 - euclidean_distance_loss: 36.8393 - val_loss: 36.5723 - val_euclidean_distance_loss: 36.1222\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.5785 - euclidean_distance_loss: 36.1276 - val_loss: 35.8577 - val_euclidean_distance_loss: 35.4055\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.9084 - euclidean_distance_loss: 35.4555 - val_loss: 35.1603 - val_euclidean_distance_loss: 34.7061\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.2062 - euclidean_distance_loss: 34.7513 - val_loss: 34.4564 - val_euclidean_distance_loss: 34.0001\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 34.4656 - euclidean_distance_loss: 34.0085 - val_loss: 33.6558 - val_euclidean_distance_loss: 33.1974\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 33.5797 - euclidean_distance_loss: 33.1205 - val_loss: 32.6943 - val_euclidean_distance_loss: 32.2336\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 32.4984 - euclidean_distance_loss: 32.0368 - val_loss: 31.5317 - val_euclidean_distance_loss: 31.0684\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 31.2004 - euclidean_distance_loss: 30.7361 - val_loss: 30.1086 - val_euclidean_distance_loss: 29.6423\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.5953 - euclidean_distance_loss: 29.1278 - val_loss: 28.3830 - val_euclidean_distance_loss: 27.9131\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.6949 - euclidean_distance_loss: 27.2235 - val_loss: 26.2691 - val_euclidean_distance_loss: 25.7948\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 25.3826 - euclidean_distance_loss: 24.9064 - val_loss: 23.8008 - val_euclidean_distance_loss: 23.3212\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 22.7537 - euclidean_distance_loss: 22.2719 - val_loss: 21.0508 - val_euclidean_distance_loss: 20.5651\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 19.9966 - euclidean_distance_loss: 19.5085 - val_loss: 18.3985 - val_euclidean_distance_loss: 17.9063\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5461 - euclidean_distance_loss: 17.0515 - val_loss: 16.2684 - val_euclidean_distance_loss: 15.7700\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 15.4794 - euclidean_distance_loss: 14.9791 - val_loss: 14.1636 - val_euclidean_distance_loss: 13.6603\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.3252 - euclidean_distance_loss: 12.8202 - val_loss: 11.9862 - val_euclidean_distance_loss: 11.4789\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11.2536 - euclidean_distance_loss: 10.7449 - val_loss: 10.2604 - val_euclidean_distance_loss: 9.7494\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.7337 - euclidean_distance_loss: 9.2214 - val_loss: 9.3075 - val_euclidean_distance_loss: 8.7930\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.7295 - euclidean_distance_loss: 8.2139 - val_loss: 8.6326 - val_euclidean_distance_loss: 8.1151\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0084 - euclidean_distance_loss: 7.4901 - val_loss: 8.0280 - val_euclidean_distance_loss: 7.5084\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4148 - euclidean_distance_loss: 6.8946 - val_loss: 7.2825 - val_euclidean_distance_loss: 6.7617\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7629 - euclidean_distance_loss: 6.2419 - val_loss: 6.6576 - val_euclidean_distance_loss: 6.1366\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2437 - euclidean_distance_loss: 5.7227 - val_loss: 6.2495 - val_euclidean_distance_loss: 5.7283\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8834 - euclidean_distance_loss: 5.3619 - val_loss: 5.9074 - val_euclidean_distance_loss: 5.3853\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6081 - euclidean_distance_loss: 5.0854 - val_loss: 5.6262 - val_euclidean_distance_loss: 5.1024\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3926 - euclidean_distance_loss: 4.8683 - val_loss: 5.4305 - val_euclidean_distance_loss: 4.9055\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2288 - euclidean_distance_loss: 4.7035 - val_loss: 5.2882 - val_euclidean_distance_loss: 4.7626\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0814 - euclidean_distance_loss: 4.5557 - val_loss: 5.1460 - val_euclidean_distance_loss: 4.6202\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9524 - euclidean_distance_loss: 4.4264 - val_loss: 5.0335 - val_euclidean_distance_loss: 4.5071\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8426 - euclidean_distance_loss: 4.3160 - val_loss: 4.9187 - val_euclidean_distance_loss: 4.3918\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7285 - euclidean_distance_loss: 4.2016 - val_loss: 4.8335 - val_euclidean_distance_loss: 4.3064\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.6361 - euclidean_distance_loss: 4.1090 - val_loss: 4.7564 - val_euclidean_distance_loss: 4.2292\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5601 - euclidean_distance_loss: 4.0327 - val_loss: 4.6539 - val_euclidean_distance_loss: 4.1263\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4782 - euclidean_distance_loss: 3.9506 - val_loss: 4.5867 - val_euclidean_distance_loss: 4.0590\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4061 - euclidean_distance_loss: 3.8783 - val_loss: 4.5319 - val_euclidean_distance_loss: 4.0039\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3369 - euclidean_distance_loss: 3.8089 - val_loss: 4.4670 - val_euclidean_distance_loss: 3.9388\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2615 - euclidean_distance_loss: 3.7333 - val_loss: 4.4185 - val_euclidean_distance_loss: 3.8902\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2156 - euclidean_distance_loss: 3.6871 - val_loss: 4.3257 - val_euclidean_distance_loss: 3.7971\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1242 - euclidean_distance_loss: 3.5955 - val_loss: 4.3166 - val_euclidean_distance_loss: 3.7879\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0771 - euclidean_distance_loss: 3.5482 - val_loss: 4.2147 - val_euclidean_distance_loss: 3.6855\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0200 - euclidean_distance_loss: 3.4906 - val_loss: 4.1719 - val_euclidean_distance_loss: 3.6424\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9668 - euclidean_distance_loss: 3.4372 - val_loss: 4.1045 - val_euclidean_distance_loss: 3.5748\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9192 - euclidean_distance_loss: 3.3894 - val_loss: 4.0829 - val_euclidean_distance_loss: 3.5531\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8955 - euclidean_distance_loss: 3.3655 - val_loss: 4.0903 - val_euclidean_distance_loss: 3.5601\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8261 - euclidean_distance_loss: 3.2958 - val_loss: 3.9728 - val_euclidean_distance_loss: 3.4425\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7636 - euclidean_distance_loss: 3.2331 - val_loss: 3.9320 - val_euclidean_distance_loss: 3.4014\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7044 - euclidean_distance_loss: 3.1738 - val_loss: 3.8581 - val_euclidean_distance_loss: 3.3274\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.6614 - euclidean_distance_loss: 3.1306 - val_loss: 3.8394 - val_euclidean_distance_loss: 3.3085\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.6385 - euclidean_distance_loss: 3.1075 - val_loss: 3.8476 - val_euclidean_distance_loss: 3.3165\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.5690 - euclidean_distance_loss: 3.0378 - val_loss: 3.7395 - val_euclidean_distance_loss: 3.2083\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5198 - euclidean_distance_loss: 2.9885 - val_loss: 3.6914 - val_euclidean_distance_loss: 3.1599\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4753 - euclidean_distance_loss: 2.9438 - val_loss: 3.6997 - val_euclidean_distance_loss: 3.1680\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.4513 - euclidean_distance_loss: 2.9196 - val_loss: 3.6381 - val_euclidean_distance_loss: 3.1063\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4239 - euclidean_distance_loss: 2.8919 - val_loss: 3.6191 - val_euclidean_distance_loss: 3.0868\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4247 - euclidean_distance_loss: 2.8925 - val_loss: 3.5784 - val_euclidean_distance_loss: 3.0463\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3430 - euclidean_distance_loss: 2.8109 - val_loss: 3.5472 - val_euclidean_distance_loss: 3.0149\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3034 - euclidean_distance_loss: 2.7710 - val_loss: 3.4995 - val_euclidean_distance_loss: 2.9670\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.2783 - euclidean_distance_loss: 2.7459 - val_loss: 3.5231 - val_euclidean_distance_loss: 2.9906\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.2492 - euclidean_distance_loss: 2.7166 - val_loss: 3.4969 - val_euclidean_distance_loss: 2.9641\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.2236 - euclidean_distance_loss: 2.6908 - val_loss: 3.4247 - val_euclidean_distance_loss: 2.8918\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1709 - euclidean_distance_loss: 2.6380 - val_loss: 3.3935 - val_euclidean_distance_loss: 2.8604\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1622 - euclidean_distance_loss: 2.6291 - val_loss: 3.3884 - val_euclidean_distance_loss: 2.8553\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.1236 - euclidean_distance_loss: 2.5904 - val_loss: 3.3667 - val_euclidean_distance_loss: 2.8333\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1059 - euclidean_distance_loss: 2.5725 - val_loss: 3.3056 - val_euclidean_distance_loss: 2.7721\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.0870 - euclidean_distance_loss: 2.5536 - val_loss: 3.3132 - val_euclidean_distance_loss: 2.7798\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0374 - euclidean_distance_loss: 2.5039 - val_loss: 3.2694 - val_euclidean_distance_loss: 2.7356\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.0114 - euclidean_distance_loss: 2.4777 - val_loss: 3.2486 - val_euclidean_distance_loss: 2.7150\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.9752 - euclidean_distance_loss: 2.4414 - val_loss: 3.2141 - val_euclidean_distance_loss: 2.6802\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.9699 - euclidean_distance_loss: 2.4359 - val_loss: 3.2204 - val_euclidean_distance_loss: 2.6864\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.9317 - euclidean_distance_loss: 2.3977 - val_loss: 3.1769 - val_euclidean_distance_loss: 2.6428\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.9064 - euclidean_distance_loss: 2.3723 - val_loss: 3.1746 - val_euclidean_distance_loss: 2.6405\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.9256 - euclidean_distance_loss: 2.3915 - val_loss: 3.2277 - val_euclidean_distance_loss: 2.6935\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.9055 - euclidean_distance_loss: 2.3710 - val_loss: 3.1006 - val_euclidean_distance_loss: 2.5659\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8639 - euclidean_distance_loss: 2.3292 - val_loss: 3.1275 - val_euclidean_distance_loss: 2.5930\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.8360 - euclidean_distance_loss: 2.3014 - val_loss: 3.0826 - val_euclidean_distance_loss: 2.5479\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.8110 - euclidean_distance_loss: 2.2763 - val_loss: 3.0697 - val_euclidean_distance_loss: 2.5348\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7872 - euclidean_distance_loss: 2.2523 - val_loss: 3.0273 - val_euclidean_distance_loss: 2.4924\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7503 - euclidean_distance_loss: 2.2155 - val_loss: 3.0234 - val_euclidean_distance_loss: 2.4886\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7331 - euclidean_distance_loss: 2.1983 - val_loss: 2.9768 - val_euclidean_distance_loss: 2.4418\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7138 - euclidean_distance_loss: 2.1787 - val_loss: 2.9707 - val_euclidean_distance_loss: 2.4355\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6978 - euclidean_distance_loss: 2.1625 - val_loss: 2.9763 - val_euclidean_distance_loss: 2.4412\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6956 - euclidean_distance_loss: 2.1604 - val_loss: 2.9496 - val_euclidean_distance_loss: 2.4141\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6737 - euclidean_distance_loss: 2.1382 - val_loss: 2.9658 - val_euclidean_distance_loss: 2.4304\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6676 - euclidean_distance_loss: 2.1322 - val_loss: 2.9392 - val_euclidean_distance_loss: 2.4037\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6641 - euclidean_distance_loss: 2.1286 - val_loss: 2.8789 - val_euclidean_distance_loss: 2.3432\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6108 - euclidean_distance_loss: 2.0750 - val_loss: 2.8563 - val_euclidean_distance_loss: 2.3205\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6079 - euclidean_distance_loss: 2.0722 - val_loss: 2.8473 - val_euclidean_distance_loss: 2.3116\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.5842 - euclidean_distance_loss: 2.0484 - val_loss: 2.8729 - val_euclidean_distance_loss: 2.3370\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.5748 - euclidean_distance_loss: 2.0389 - val_loss: 2.8295 - val_euclidean_distance_loss: 2.2936\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5678 - euclidean_distance_loss: 2.0320 - val_loss: 2.8319 - val_euclidean_distance_loss: 2.2959\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.5578 - euclidean_distance_loss: 2.0219 - val_loss: 2.7810 - val_euclidean_distance_loss: 2.2450\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5351 - euclidean_distance_loss: 1.9990 - val_loss: 2.8291 - val_euclidean_distance_loss: 2.2931\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.5381 - euclidean_distance_loss: 2.0020 - val_loss: 2.7958 - val_euclidean_distance_loss: 2.2596\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4987 - euclidean_distance_loss: 1.9626 - val_loss: 2.7650 - val_euclidean_distance_loss: 2.2288\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.4981 - euclidean_distance_loss: 1.9619 - val_loss: 2.7595 - val_euclidean_distance_loss: 2.2235\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4896 - euclidean_distance_loss: 1.9534 - val_loss: 2.7302 - val_euclidean_distance_loss: 2.1939\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4779 - euclidean_distance_loss: 1.9418 - val_loss: 2.7605 - val_euclidean_distance_loss: 2.2243\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.4506 - euclidean_distance_loss: 1.9143 - val_loss: 2.6850 - val_euclidean_distance_loss: 2.1486\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.4285 - euclidean_distance_loss: 1.8921 - val_loss: 2.6771 - val_euclidean_distance_loss: 2.1408\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.4063 - euclidean_distance_loss: 1.8700 - val_loss: 2.6553 - val_euclidean_distance_loss: 2.1190\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.4037 - euclidean_distance_loss: 1.8673 - val_loss: 2.6480 - val_euclidean_distance_loss: 2.1115\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3987 - euclidean_distance_loss: 1.8623 - val_loss: 2.6402 - val_euclidean_distance_loss: 2.1038\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3747 - euclidean_distance_loss: 1.8383 - val_loss: 2.6250 - val_euclidean_distance_loss: 2.0885\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3849 - euclidean_distance_loss: 1.8483 - val_loss: 2.6092 - val_euclidean_distance_loss: 2.0727\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3893 - euclidean_distance_loss: 1.8528 - val_loss: 2.6148 - val_euclidean_distance_loss: 2.0783\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3443 - euclidean_distance_loss: 1.8077 - val_loss: 2.5970 - val_euclidean_distance_loss: 2.0603\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.3387 - euclidean_distance_loss: 1.8021 - val_loss: 2.5716 - val_euclidean_distance_loss: 2.0350\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.3246 - euclidean_distance_loss: 1.7881 - val_loss: 2.5984 - val_euclidean_distance_loss: 2.0620\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.3271 - euclidean_distance_loss: 1.7906 - val_loss: 2.5951 - val_euclidean_distance_loss: 2.0585\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.3079 - euclidean_distance_loss: 1.7712 - val_loss: 2.5901 - val_euclidean_distance_loss: 2.0535\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2938 - euclidean_distance_loss: 1.7574 - val_loss: 2.5455 - val_euclidean_distance_loss: 2.0091\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2812 - euclidean_distance_loss: 1.7447 - val_loss: 2.5587 - val_euclidean_distance_loss: 2.0220\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2929 - euclidean_distance_loss: 1.7562 - val_loss: 2.5662 - val_euclidean_distance_loss: 2.0294\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3028 - euclidean_distance_loss: 1.7662 - val_loss: 2.5698 - val_euclidean_distance_loss: 2.0331\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3167 - euclidean_distance_loss: 1.7799 - val_loss: 2.5468 - val_euclidean_distance_loss: 2.0101\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3074 - euclidean_distance_loss: 1.7709 - val_loss: 2.5068 - val_euclidean_distance_loss: 1.9703\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2781 - euclidean_distance_loss: 1.7414 - val_loss: 2.5761 - val_euclidean_distance_loss: 2.0394\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2555 - euclidean_distance_loss: 1.7189 - val_loss: 2.5180 - val_euclidean_distance_loss: 1.9815\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2456 - euclidean_distance_loss: 1.7091 - val_loss: 2.4714 - val_euclidean_distance_loss: 1.9347\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2338 - euclidean_distance_loss: 1.6972 - val_loss: 2.4703 - val_euclidean_distance_loss: 1.9338\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2188 - euclidean_distance_loss: 1.6823 - val_loss: 2.4868 - val_euclidean_distance_loss: 1.9502\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2069 - euclidean_distance_loss: 1.6704 - val_loss: 2.4307 - val_euclidean_distance_loss: 1.8941\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.1870 - euclidean_distance_loss: 1.6505 - val_loss: 2.4405 - val_euclidean_distance_loss: 1.9040\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.1854 - euclidean_distance_loss: 1.6489 - val_loss: 2.4351 - val_euclidean_distance_loss: 1.8986\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1863 - euclidean_distance_loss: 1.6498 - val_loss: 2.4077 - val_euclidean_distance_loss: 1.8712\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1647 - euclidean_distance_loss: 1.6282 - val_loss: 2.3891 - val_euclidean_distance_loss: 1.8525\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1419 - euclidean_distance_loss: 1.6054 - val_loss: 2.4092 - val_euclidean_distance_loss: 1.8728\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1466 - euclidean_distance_loss: 1.6101 - val_loss: 2.4545 - val_euclidean_distance_loss: 1.9180\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1590 - euclidean_distance_loss: 1.6225 - val_loss: 2.3949 - val_euclidean_distance_loss: 1.8586\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1248 - euclidean_distance_loss: 1.5884 - val_loss: 2.3813 - val_euclidean_distance_loss: 1.8448\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.1394 - euclidean_distance_loss: 1.6029 - val_loss: 2.3663 - val_euclidean_distance_loss: 1.8299\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1282 - euclidean_distance_loss: 1.5919 - val_loss: 2.3698 - val_euclidean_distance_loss: 1.8336\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.1157 - euclidean_distance_loss: 1.5793 - val_loss: 2.4153 - val_euclidean_distance_loss: 1.8790\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.1193 - euclidean_distance_loss: 1.5831 - val_loss: 2.4359 - val_euclidean_distance_loss: 1.8998\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.1302 - euclidean_distance_loss: 1.5940 - val_loss: 2.3940 - val_euclidean_distance_loss: 1.8577\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1002 - euclidean_distance_loss: 1.5640 - val_loss: 2.3383 - val_euclidean_distance_loss: 1.8022\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0735 - euclidean_distance_loss: 1.5374 - val_loss: 2.3179 - val_euclidean_distance_loss: 1.7818\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0740 - euclidean_distance_loss: 1.5378 - val_loss: 2.3843 - val_euclidean_distance_loss: 1.8482\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1068 - euclidean_distance_loss: 1.5708 - val_loss: 2.3228 - val_euclidean_distance_loss: 1.7867\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0644 - euclidean_distance_loss: 1.5281 - val_loss: 2.3513 - val_euclidean_distance_loss: 1.8152\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0733 - euclidean_distance_loss: 1.5373 - val_loss: 2.2870 - val_euclidean_distance_loss: 1.7509\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0475 - euclidean_distance_loss: 1.5115 - val_loss: 2.2856 - val_euclidean_distance_loss: 1.7495\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0509 - euclidean_distance_loss: 1.5149 - val_loss: 2.2831 - val_euclidean_distance_loss: 1.7471\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0366 - euclidean_distance_loss: 1.5006 - val_loss: 2.2722 - val_euclidean_distance_loss: 1.7363\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0326 - euclidean_distance_loss: 1.4965 - val_loss: 2.2780 - val_euclidean_distance_loss: 1.7422\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0323 - euclidean_distance_loss: 1.4966 - val_loss: 2.2613 - val_euclidean_distance_loss: 1.7255\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0345 - euclidean_distance_loss: 1.4985 - val_loss: 2.2673 - val_euclidean_distance_loss: 1.7314\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9982 - euclidean_distance_loss: 1.4624 - val_loss: 2.2968 - val_euclidean_distance_loss: 1.7610\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0268 - euclidean_distance_loss: 1.4911 - val_loss: 2.2564 - val_euclidean_distance_loss: 1.7207\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0169 - euclidean_distance_loss: 1.4812 - val_loss: 2.2854 - val_euclidean_distance_loss: 1.7497\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0048 - euclidean_distance_loss: 1.4691 - val_loss: 2.2462 - val_euclidean_distance_loss: 1.7106\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0022 - euclidean_distance_loss: 1.4667 - val_loss: 2.2674 - val_euclidean_distance_loss: 1.7317\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9868 - euclidean_distance_loss: 1.4511 - val_loss: 2.2260 - val_euclidean_distance_loss: 1.6904\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9864 - euclidean_distance_loss: 1.4510 - val_loss: 2.2306 - val_euclidean_distance_loss: 1.6951\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9668 - euclidean_distance_loss: 1.4313 - val_loss: 2.2182 - val_euclidean_distance_loss: 1.6828\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9472 - euclidean_distance_loss: 1.4118 - val_loss: 2.1981 - val_euclidean_distance_loss: 1.6626\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9643 - euclidean_distance_loss: 1.4288 - val_loss: 2.2554 - val_euclidean_distance_loss: 1.7202\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9904 - euclidean_distance_loss: 1.4551 - val_loss: 2.1973 - val_euclidean_distance_loss: 1.6618\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9693 - euclidean_distance_loss: 1.4340 - val_loss: 2.2467 - val_euclidean_distance_loss: 1.7115\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9731 - euclidean_distance_loss: 1.4379 - val_loss: 2.1761 - val_euclidean_distance_loss: 1.6408\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9480 - euclidean_distance_loss: 1.4129 - val_loss: 2.2075 - val_euclidean_distance_loss: 1.6724\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9558 - euclidean_distance_loss: 1.4206 - val_loss: 2.1835 - val_euclidean_distance_loss: 1.6484\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9364 - euclidean_distance_loss: 1.4014 - val_loss: 2.1763 - val_euclidean_distance_loss: 1.6413\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9337 - euclidean_distance_loss: 1.3988 - val_loss: 2.1896 - val_euclidean_distance_loss: 1.6547\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9160 - euclidean_distance_loss: 1.3809 - val_loss: 2.1746 - val_euclidean_distance_loss: 1.6396\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9223 - euclidean_distance_loss: 1.3876 - val_loss: 2.2418 - val_euclidean_distance_loss: 1.7069\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9409 - euclidean_distance_loss: 1.4059 - val_loss: 2.1859 - val_euclidean_distance_loss: 1.6512\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9196 - euclidean_distance_loss: 1.3849 - val_loss: 2.1468 - val_euclidean_distance_loss: 1.6118\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9275 - euclidean_distance_loss: 1.3926 - val_loss: 2.2046 - val_euclidean_distance_loss: 1.6700\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9564 - euclidean_distance_loss: 1.4217 - val_loss: 2.1610 - val_euclidean_distance_loss: 1.6262\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9125 - euclidean_distance_loss: 1.3778 - val_loss: 2.1638 - val_euclidean_distance_loss: 1.6292\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9023 - euclidean_distance_loss: 1.3676 - val_loss: 2.1315 - val_euclidean_distance_loss: 1.5969\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8847 - euclidean_distance_loss: 1.3503 - val_loss: 2.1404 - val_euclidean_distance_loss: 1.6059\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9087 - euclidean_distance_loss: 1.3743 - val_loss: 2.1736 - val_euclidean_distance_loss: 1.6391\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8901 - euclidean_distance_loss: 1.3556 - val_loss: 2.1487 - val_euclidean_distance_loss: 1.6143\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8629 - euclidean_distance_loss: 1.3287 - val_loss: 2.1230 - val_euclidean_distance_loss: 1.5886\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8655 - euclidean_distance_loss: 1.3312 - val_loss: 2.1250 - val_euclidean_distance_loss: 1.5908\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8543 - euclidean_distance_loss: 1.3201 - val_loss: 2.1774 - val_euclidean_distance_loss: 1.6432\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8684 - euclidean_distance_loss: 1.3341 - val_loss: 2.1092 - val_euclidean_distance_loss: 1.5751\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8723 - euclidean_distance_loss: 1.3382 - val_loss: 2.1030 - val_euclidean_distance_loss: 1.5689\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8529 - euclidean_distance_loss: 1.3188 - val_loss: 2.1163 - val_euclidean_distance_loss: 1.5824\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.8766 - euclidean_distance_loss: 1.3427 - val_loss: 2.1176 - val_euclidean_distance_loss: 1.5836\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8525 - euclidean_distance_loss: 1.3185 - val_loss: 2.0851 - val_euclidean_distance_loss: 1.5512\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8305 - euclidean_distance_loss: 1.2967 - val_loss: 2.0683 - val_euclidean_distance_loss: 1.5344\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8557 - euclidean_distance_loss: 1.3219 - val_loss: 2.0890 - val_euclidean_distance_loss: 1.5552\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8468 - euclidean_distance_loss: 1.3130 - val_loss: 2.0957 - val_euclidean_distance_loss: 1.5621\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8173 - euclidean_distance_loss: 1.2836 - val_loss: 2.0671 - val_euclidean_distance_loss: 1.5335\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8201 - euclidean_distance_loss: 1.2865 - val_loss: 2.0785 - val_euclidean_distance_loss: 1.5452\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8385 - euclidean_distance_loss: 1.3050 - val_loss: 2.0675 - val_euclidean_distance_loss: 1.5341\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8076 - euclidean_distance_loss: 1.2742 - val_loss: 2.1122 - val_euclidean_distance_loss: 1.5788\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8607 - euclidean_distance_loss: 1.3273 - val_loss: 2.0577 - val_euclidean_distance_loss: 1.5244\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8421 - euclidean_distance_loss: 1.3089 - val_loss: 2.0917 - val_euclidean_distance_loss: 1.5584\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8162 - euclidean_distance_loss: 1.2830 - val_loss: 2.0677 - val_euclidean_distance_loss: 1.5346\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8048 - euclidean_distance_loss: 1.2716 - val_loss: 2.0298 - val_euclidean_distance_loss: 1.4967\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7869 - euclidean_distance_loss: 1.2540 - val_loss: 2.0190 - val_euclidean_distance_loss: 1.4860\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7811 - euclidean_distance_loss: 1.2481 - val_loss: 2.1065 - val_euclidean_distance_loss: 1.5737\n",
      "Epoch 214/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8106 - euclidean_distance_loss: 1.2777 - val_loss: 2.0282 - val_euclidean_distance_loss: 1.4952\n",
      "Epoch 215/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7826 - euclidean_distance_loss: 1.2498 - val_loss: 2.0794 - val_euclidean_distance_loss: 1.5467\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7803 - euclidean_distance_loss: 1.2475 - val_loss: 2.0207 - val_euclidean_distance_loss: 1.4878\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7539 - euclidean_distance_loss: 1.2212 - val_loss: 2.0277 - val_euclidean_distance_loss: 1.4951\n",
      "Epoch 218/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7590 - euclidean_distance_loss: 1.2265 - val_loss: 2.0008 - val_euclidean_distance_loss: 1.4682\n",
      "Epoch 219/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7907 - euclidean_distance_loss: 1.2581 - val_loss: 2.0577 - val_euclidean_distance_loss: 1.5254\n",
      "Epoch 220/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7849 - euclidean_distance_loss: 1.2526 - val_loss: 2.0299 - val_euclidean_distance_loss: 1.4974\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7664 - euclidean_distance_loss: 1.2340 - val_loss: 2.0264 - val_euclidean_distance_loss: 1.4941\n",
      "Epoch 222/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7810 - euclidean_distance_loss: 1.2488 - val_loss: 2.0044 - val_euclidean_distance_loss: 1.4721\n",
      "Epoch 223/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7601 - euclidean_distance_loss: 1.2278 - val_loss: 2.0037 - val_euclidean_distance_loss: 1.4716\n",
      "Epoch 224/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7460 - euclidean_distance_loss: 1.2139 - val_loss: 2.0424 - val_euclidean_distance_loss: 1.5104\n",
      "Epoch 225/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7633 - euclidean_distance_loss: 1.2313 - val_loss: 1.9927 - val_euclidean_distance_loss: 1.4606\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7389 - euclidean_distance_loss: 1.2071 - val_loss: 1.9761 - val_euclidean_distance_loss: 1.4444\n",
      "Epoch 227/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7414 - euclidean_distance_loss: 1.2096 - val_loss: 2.0293 - val_euclidean_distance_loss: 1.4973\n",
      "Epoch 228/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7642 - euclidean_distance_loss: 1.2324 - val_loss: 2.0522 - val_euclidean_distance_loss: 1.5204\n",
      "Epoch 229/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7667 - euclidean_distance_loss: 1.2350 - val_loss: 1.9940 - val_euclidean_distance_loss: 1.4624\n",
      "Epoch 230/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7243 - euclidean_distance_loss: 1.1927 - val_loss: 1.9707 - val_euclidean_distance_loss: 1.4391\n",
      "Epoch 231/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7280 - euclidean_distance_loss: 1.1966 - val_loss: 1.9966 - val_euclidean_distance_loss: 1.4652\n",
      "Epoch 232/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7176 - euclidean_distance_loss: 1.1861 - val_loss: 1.9689 - val_euclidean_distance_loss: 1.4374\n",
      "Epoch 233/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7066 - euclidean_distance_loss: 1.1752 - val_loss: 1.9709 - val_euclidean_distance_loss: 1.4396\n",
      "Epoch 234/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6952 - euclidean_distance_loss: 1.1640 - val_loss: 2.0138 - val_euclidean_distance_loss: 1.4825\n",
      "Epoch 235/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7360 - euclidean_distance_loss: 1.2048 - val_loss: 1.9685 - val_euclidean_distance_loss: 1.4374\n",
      "Epoch 236/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7279 - euclidean_distance_loss: 1.1968 - val_loss: 1.9636 - val_euclidean_distance_loss: 1.4325\n",
      "Epoch 237/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7035 - euclidean_distance_loss: 1.1726 - val_loss: 1.9506 - val_euclidean_distance_loss: 1.4195\n",
      "Epoch 238/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7118 - euclidean_distance_loss: 1.1807 - val_loss: 1.9927 - val_euclidean_distance_loss: 1.4618\n",
      "Epoch 239/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7033 - euclidean_distance_loss: 1.1724 - val_loss: 1.9590 - val_euclidean_distance_loss: 1.4282\n",
      "Epoch 240/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6999 - euclidean_distance_loss: 1.1690 - val_loss: 1.9538 - val_euclidean_distance_loss: 1.4232\n",
      "Epoch 241/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7052 - euclidean_distance_loss: 1.1746 - val_loss: 1.9890 - val_euclidean_distance_loss: 1.4586\n",
      "Epoch 242/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7134 - euclidean_distance_loss: 1.1829 - val_loss: 1.9436 - val_euclidean_distance_loss: 1.4130\n",
      "Epoch 243/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6881 - euclidean_distance_loss: 1.1578 - val_loss: 2.0231 - val_euclidean_distance_loss: 1.4928\n",
      "Epoch 244/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7188 - euclidean_distance_loss: 1.1884 - val_loss: 1.9283 - val_euclidean_distance_loss: 1.3979\n",
      "Epoch 245/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6756 - euclidean_distance_loss: 1.1453 - val_loss: 1.9694 - val_euclidean_distance_loss: 1.4390\n",
      "Epoch 246/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6876 - euclidean_distance_loss: 1.1573 - val_loss: 1.9898 - val_euclidean_distance_loss: 1.4598\n",
      "Epoch 247/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6812 - euclidean_distance_loss: 1.1511 - val_loss: 1.9218 - val_euclidean_distance_loss: 1.3917\n",
      "Epoch 248/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6601 - euclidean_distance_loss: 1.1302 - val_loss: 1.9123 - val_euclidean_distance_loss: 1.3822\n",
      "Epoch 249/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6591 - euclidean_distance_loss: 1.1291 - val_loss: 1.9134 - val_euclidean_distance_loss: 1.3833\n",
      "Epoch 250/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6557 - euclidean_distance_loss: 1.1257 - val_loss: 1.9105 - val_euclidean_distance_loss: 1.3806\n",
      "Epoch 251/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6365 - euclidean_distance_loss: 1.1065 - val_loss: 1.9230 - val_euclidean_distance_loss: 1.3931\n",
      "Epoch 252/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6617 - euclidean_distance_loss: 1.1320 - val_loss: 1.9219 - val_euclidean_distance_loss: 1.3922\n",
      "Epoch 253/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6404 - euclidean_distance_loss: 1.1107 - val_loss: 1.8836 - val_euclidean_distance_loss: 1.3540\n",
      "Epoch 254/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6251 - euclidean_distance_loss: 1.0955 - val_loss: 1.8951 - val_euclidean_distance_loss: 1.3656\n",
      "Epoch 255/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6466 - euclidean_distance_loss: 1.1172 - val_loss: 1.8813 - val_euclidean_distance_loss: 1.3519\n",
      "Epoch 256/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6329 - euclidean_distance_loss: 1.1035 - val_loss: 1.8914 - val_euclidean_distance_loss: 1.3621\n",
      "Epoch 257/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6257 - euclidean_distance_loss: 1.0965 - val_loss: 1.9046 - val_euclidean_distance_loss: 1.3753\n",
      "Epoch 258/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6527 - euclidean_distance_loss: 1.1235 - val_loss: 1.8695 - val_euclidean_distance_loss: 1.3403\n",
      "Epoch 259/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6178 - euclidean_distance_loss: 1.0888 - val_loss: 1.8735 - val_euclidean_distance_loss: 1.3446\n",
      "Epoch 260/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6234 - euclidean_distance_loss: 1.0944 - val_loss: 1.9414 - val_euclidean_distance_loss: 1.4125\n",
      "Epoch 261/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6549 - euclidean_distance_loss: 1.1260 - val_loss: 1.8931 - val_euclidean_distance_loss: 1.3642\n",
      "Epoch 262/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6381 - euclidean_distance_loss: 1.1093 - val_loss: 1.8859 - val_euclidean_distance_loss: 1.3571\n",
      "Epoch 263/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6348 - euclidean_distance_loss: 1.1059 - val_loss: 1.8874 - val_euclidean_distance_loss: 1.3586\n",
      "Epoch 264/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6239 - euclidean_distance_loss: 1.0952 - val_loss: 1.8651 - val_euclidean_distance_loss: 1.3365\n",
      "Epoch 265/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6375 - euclidean_distance_loss: 1.1089 - val_loss: 1.8992 - val_euclidean_distance_loss: 1.3705\n",
      "Epoch 266/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6467 - euclidean_distance_loss: 1.1182 - val_loss: 1.9006 - val_euclidean_distance_loss: 1.3723\n",
      "Epoch 267/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6090 - euclidean_distance_loss: 1.0806 - val_loss: 1.8920 - val_euclidean_distance_loss: 1.3636\n",
      "Epoch 268/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6282 - euclidean_distance_loss: 1.0998 - val_loss: 1.9050 - val_euclidean_distance_loss: 1.3766\n",
      "Epoch 269/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6313 - euclidean_distance_loss: 1.1030 - val_loss: 1.8671 - val_euclidean_distance_loss: 1.3390\n",
      "Epoch 270/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6248 - euclidean_distance_loss: 1.0965 - val_loss: 1.8718 - val_euclidean_distance_loss: 1.3436\n",
      "Epoch 271/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6265 - euclidean_distance_loss: 1.0985 - val_loss: 1.8513 - val_euclidean_distance_loss: 1.3232\n",
      "Epoch 272/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6143 - euclidean_distance_loss: 1.0861 - val_loss: 1.9501 - val_euclidean_distance_loss: 1.4223\n",
      "Epoch 273/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6915 - euclidean_distance_loss: 1.1635 - val_loss: 1.8625 - val_euclidean_distance_loss: 1.3345\n",
      "Epoch 274/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6187 - euclidean_distance_loss: 1.0909 - val_loss: 1.8629 - val_euclidean_distance_loss: 1.3350\n",
      "Epoch 275/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6372 - euclidean_distance_loss: 1.1093 - val_loss: 1.9105 - val_euclidean_distance_loss: 1.3829\n",
      "Epoch 276/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6186 - euclidean_distance_loss: 1.0908 - val_loss: 1.8407 - val_euclidean_distance_loss: 1.3130\n",
      "Epoch 277/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6026 - euclidean_distance_loss: 1.0750 - val_loss: 1.9341 - val_euclidean_distance_loss: 1.4065\n",
      "Epoch 278/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6382 - euclidean_distance_loss: 1.1105 - val_loss: 1.8695 - val_euclidean_distance_loss: 1.3420\n",
      "Epoch 279/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6040 - euclidean_distance_loss: 1.0766 - val_loss: 1.8506 - val_euclidean_distance_loss: 1.3232\n",
      "Epoch 280/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5912 - euclidean_distance_loss: 1.0638 - val_loss: 1.8323 - val_euclidean_distance_loss: 1.3050\n",
      "Epoch 281/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6147 - euclidean_distance_loss: 1.0875 - val_loss: 1.9187 - val_euclidean_distance_loss: 1.3915\n",
      "Epoch 282/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6385 - euclidean_distance_loss: 1.1112 - val_loss: 1.8413 - val_euclidean_distance_loss: 1.3143\n",
      "Epoch 283/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6181 - euclidean_distance_loss: 1.0910 - val_loss: 1.8498 - val_euclidean_distance_loss: 1.3226\n",
      "Epoch 284/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5876 - euclidean_distance_loss: 1.0606 - val_loss: 1.8658 - val_euclidean_distance_loss: 1.3389\n",
      "Epoch 285/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6122 - euclidean_distance_loss: 1.0852 - val_loss: 1.8568 - val_euclidean_distance_loss: 1.3300\n",
      "Epoch 286/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5801 - euclidean_distance_loss: 1.0534 - val_loss: 1.8167 - val_euclidean_distance_loss: 1.2899\n",
      "Epoch 287/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5702 - euclidean_distance_loss: 1.0435 - val_loss: 1.8403 - val_euclidean_distance_loss: 1.3136\n",
      "Epoch 288/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5777 - euclidean_distance_loss: 1.0511 - val_loss: 1.8419 - val_euclidean_distance_loss: 1.3155\n",
      "Epoch 289/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5558 - euclidean_distance_loss: 1.0294 - val_loss: 1.8207 - val_euclidean_distance_loss: 1.2943\n",
      "Epoch 290/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5632 - euclidean_distance_loss: 1.0368 - val_loss: 1.7887 - val_euclidean_distance_loss: 1.2623\n",
      "Epoch 291/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5580 - euclidean_distance_loss: 1.0317 - val_loss: 1.8167 - val_euclidean_distance_loss: 1.2905\n",
      "Epoch 292/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.5670 - euclidean_distance_loss: 1.0408 - val_loss: 1.8588 - val_euclidean_distance_loss: 1.3327\n",
      "Epoch 293/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.5905 - euclidean_distance_loss: 1.0644 - val_loss: 1.9423 - val_euclidean_distance_loss: 1.4162\n",
      "Epoch 294/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6299 - euclidean_distance_loss: 1.1038 - val_loss: 1.8420 - val_euclidean_distance_loss: 1.3159\n",
      "Epoch 295/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6047 - euclidean_distance_loss: 1.0787 - val_loss: 1.8589 - val_euclidean_distance_loss: 1.3329\n",
      "Epoch 296/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.5850 - euclidean_distance_loss: 1.0589 - val_loss: 1.9336 - val_euclidean_distance_loss: 1.4077\n",
      "Epoch 297/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6048 - euclidean_distance_loss: 1.0790 - val_loss: 1.8845 - val_euclidean_distance_loss: 1.3586\n",
      "Epoch 298/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.5809 - euclidean_distance_loss: 1.0551 - val_loss: 1.8089 - val_euclidean_distance_loss: 1.2833\n",
      "Epoch 299/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.5165 - euclidean_distance_loss: 0.9909 - val_loss: 1.7708 - val_euclidean_distance_loss: 1.2452\n",
      "Epoch 300/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.5551 - euclidean_distance_loss: 1.0297 - val_loss: 1.8003 - val_euclidean_distance_loss: 1.2746\n",
      "5/5 [==============================] - 0s 922us/step\n",
      "29/29 [==============================] - 0s 499us/step\n",
      "TR Loss:  1.5550999641418457\n",
      "VL Loss:  1.8002616167068481\n",
      "TS Loss:  1.3191324472427368\n",
      "Predictions for X:  [ 9.45552444e+00  8.68444443e+00  9.78196049e+00 -1.37878733e+01\n",
      " -4.91934662e+01  9.31649685e+00  9.54255009e+00  8.36586189e+00\n",
      "  4.77857733e+00 -3.84943657e+01  5.62454367e+00  9.14420986e+00\n",
      " -1.11368742e+01 -2.37276230e+01 -6.83687820e+01  9.61332607e+00\n",
      " -5.84721851e+00  9.64033890e+00  9.61185074e+00  8.19407940e+00\n",
      "  6.85853767e+00  8.73340416e+00 -4.77937431e+01 -7.00415802e+01\n",
      "  5.99520016e+00  9.89140415e+00  7.91420650e+00  5.22446823e+00\n",
      "  9.74999046e+00  9.61554146e+00  1.00016699e+01  4.58953762e+00\n",
      "  5.32225418e+00 -5.89676905e+00 -6.36696358e+01 -1.06091862e+01\n",
      "  5.87426758e+00 -5.59133873e+01  2.05775762e+00  7.74874592e+00\n",
      " -6.93174896e+01  3.93400788e-01  3.55437255e+00  7.57132816e+00\n",
      "  6.77223492e+00  7.80128193e+00 -6.40210807e-01  2.40394044e+00\n",
      "  5.96888971e+00  9.29841137e+00 -7.11496201e+01 -3.99063034e+01\n",
      " -5.38973160e+01 -6.62154083e+01 -1.06559539e+00  4.67482328e+00\n",
      "  9.39969540e+00  9.14408016e+00  4.96355772e+00 -4.73012886e+01\n",
      "  5.69650078e+00  5.63051367e+00  8.99233437e+00  9.73772240e+00\n",
      " -3.96093407e+01 -2.97152882e+01  5.56577826e+00 -3.76940918e+01\n",
      "  9.38981915e+00 -3.82574272e+01 -5.86561165e+01  1.00211210e+01\n",
      "  8.99271202e+00 -3.10888481e+00  7.61672115e+00 -7.09039917e+01\n",
      "  1.07638636e+01 -2.38421745e+01  9.55123615e+00  8.30932903e+00\n",
      " -6.17612953e+01 -6.18878479e+01  9.49589062e+00  9.28367710e+00\n",
      "  9.63864803e+00 -6.86788330e+01 -3.17157974e+01  7.11640644e+00\n",
      "  1.06110239e+01 -1.76955452e+01 -1.86866283e+01  9.66945076e+00\n",
      " -6.39361229e+01 -5.69375267e+01 -3.82729836e+01 -1.14401472e+00\n",
      "  4.54023504e+00  1.01423502e+01 -6.58294907e+01  2.52827382e+00\n",
      "  8.50670910e+00  9.04028988e+00  9.41199589e+00 -7.35326862e+00\n",
      "  1.77105367e+00  9.20454407e+00  9.72946835e+00 -5.19806404e+01\n",
      " -6.92611160e+01 -8.51410580e+00 -6.53610382e+01  9.70453072e+00\n",
      "  9.61582088e+00 -6.90870056e+01  5.05147839e+00 -6.25202637e+01\n",
      "  3.78250957e-01  9.44345951e+00 -2.65528107e+01  9.19036579e+00\n",
      " -5.41861076e+01  9.04934406e+00 -7.77293968e+00 -6.73839951e+01\n",
      "  2.19377494e+00  5.60042953e+00 -1.01025839e+01  9.23035622e+00\n",
      "  6.60529709e+00  9.51183414e+00  9.65790749e+00 -6.76005859e+01\n",
      "  8.61518574e+00  3.35385466e+00 -2.59562759e+01  9.03557968e+00\n",
      " -4.58329315e+01 -6.82732544e+01 -5.85095787e+01 -7.24789762e+00\n",
      " -5.17418327e+01  7.83012676e+00  1.00948038e+01 -6.93146210e+01\n",
      "  8.06592369e+00 -6.25445976e+01  9.23984051e+00 -2.99371414e+01\n",
      " -5.62536964e+01 -7.11008759e+01 -6.64944839e+01  9.71720409e+00\n",
      "  7.88853025e+00  5.84846020e+00  6.22151756e+00  9.57572746e+00\n",
      "  5.48225975e+00  8.09478188e+00  4.64418745e+00 -6.21153717e+01\n",
      " -6.45569086e+00 -1.21631498e+01  9.53396702e+00 -1.86096878e+01\n",
      " -2.44426136e+01  7.03771305e+00 -6.69395905e+01 -6.13366432e+01\n",
      " -6.59753876e+01 -4.40793724e+01 -3.87863197e+01 -6.71427765e+01\n",
      " -3.11127892e+01 -4.12348366e+01 -1.74986992e+01  2.89347291e+00\n",
      " -5.74235764e+01  1.01050310e+01  9.28986740e+00 -3.46938438e+01\n",
      "  7.98202038e+00  9.74814701e+00 -6.37251015e+01 -4.67907810e+00\n",
      " -2.26651211e+01  5.87238264e+00 -4.66389542e+01 -1.08527918e+01\n",
      " -1.34550781e+01 -6.16444397e+01  8.99028015e+00  8.88738251e+00\n",
      " -2.87846451e+01 -7.03999329e+01  8.53382683e+00  9.25257206e+00\n",
      "  9.50193119e+00 -6.71239319e+01  8.89113712e+00 -1.19334245e+00\n",
      "  9.20691872e+00  8.20009232e+00 -6.92183304e+01 -9.56323338e+00\n",
      "  8.40961170e+00  4.09554672e+00 -5.63015327e+01 -3.65873337e+01\n",
      " -1.31848803e+01 -6.83002853e+01  9.48217678e+00  1.06894481e+00\n",
      "  9.73099995e+00 -7.88471746e+00  8.97948170e+00  4.89579964e+00\n",
      " -3.07311707e+01 -3.73017845e+01  9.69674110e+00  1.00805912e+01\n",
      "  9.55287838e+00  5.04196501e+00 -7.02408600e+01 -3.11257572e+01\n",
      " -7.03582153e+01 -7.04141312e+01 -3.09592628e+01  9.91611290e+00\n",
      "  9.42331791e+00 -5.13296204e+01  7.94538498e-01 -2.33389759e+01\n",
      "  7.86638737e+00 -1.38770924e+01 -6.00425911e+01 -6.86591492e+01\n",
      " -2.30493031e+01 -3.12558699e+00  8.25953007e+00  9.68550396e+00\n",
      " -7.17174683e+01  1.51460302e+00 -4.11225853e+01  9.48357010e+00\n",
      " -6.48146896e+01 -2.61273193e+01  8.99486542e+00 -7.11666336e+01\n",
      " -5.52808495e+01  1.01893444e+01 -5.19776726e+00 -1.54135313e+01\n",
      "  9.71877670e+00  8.99362183e+00  7.98067760e+00  3.55177808e+00\n",
      " -2.61877441e+01  2.65700698e-01  1.05977545e+01  9.00481415e+00\n",
      " -9.61541367e+00  2.80067897e+00  1.05055723e+01  5.42093849e+00\n",
      "  9.64686108e+00  1.04540567e+01 -3.89467659e+01 -1.20967665e+01\n",
      " -4.61266060e+01  6.84530926e+00  8.13073921e+00  1.07810812e+01\n",
      " -5.08281250e+01 -6.43809204e+01  1.96001375e+00 -3.43941574e+01\n",
      "  9.79049397e+00  1.14632833e+00  5.44358253e+00  8.78433323e+00\n",
      "  9.70194054e+00 -2.17072372e+01 -3.91272545e+01 -6.97803040e+01\n",
      "  5.89234924e+00 -7.12339020e+01 -3.92974129e+01 -5.13196411e+01\n",
      "  1.00993938e+01  9.64235401e+00 -1.20237770e+01  9.54939842e+00\n",
      " -7.01627197e+01  8.95907593e+00  9.24363518e+00  4.76864910e+00\n",
      "  8.80665398e+00 -6.41881180e+01  9.88538170e+00  7.93560219e+00\n",
      "  6.52795792e+00  7.04341698e+00  9.71341801e+00 -1.85460615e+00\n",
      " -2.12608032e+01 -1.20217829e+01  8.89741421e+00  1.07701979e+01\n",
      " -7.13583450e+01 -2.70679111e+01 -2.58033886e+01  4.42201710e+00\n",
      "  9.55283833e+00  8.90694618e+00  3.37673688e+00  8.59126949e+00\n",
      "  6.20991325e+00 -1.61292191e+01  1.00218430e+01 -6.26378899e+01\n",
      "  9.12084484e+00  9.89729118e+00  8.21363747e-01 -7.04263840e+01\n",
      " -1.06476202e+01  9.33860207e+00  9.70032406e+00 -1.15190954e+01\n",
      " -4.40459862e+01 -1.05187283e+01 -5.55526390e+01  9.69904137e+00\n",
      " -2.82995300e+01  7.76323509e+00  8.70754719e+00  9.80460262e+00\n",
      "  1.13639390e+00  8.96892071e+00 -3.73318911e+00 -1.46579523e+01\n",
      "  9.71769333e+00 -3.96537170e+01 -5.27290306e+01 -5.50470543e+01\n",
      "  9.70240116e+00  1.04605494e+01  2.77849174e+00  1.61050403e+00\n",
      "  9.63959408e+00  8.18548584e+00 -7.04180450e+01  9.78481102e+00\n",
      " -6.58738251e+01 -5.59983063e+01 -7.13112183e+01 -4.47615776e+01\n",
      " -2.10939903e+01  9.06777477e+00 -2.92739677e+01  9.68493748e+00\n",
      " -3.26694984e+01 -1.51007385e+01 -3.22685051e+01 -3.83310204e+01\n",
      " -5.42374001e+01 -4.50036964e+01  9.44387817e+00  7.66335392e+00\n",
      "  9.47030354e+00  8.96199799e+00 -5.93034630e+01 -7.68238831e+00\n",
      "  7.21126175e+00  5.83933020e+00  9.96300030e+00 -3.95262871e+01\n",
      " -3.14058132e+01  1.02002401e+01 -5.75061607e+01 -1.14097090e+01\n",
      " -5.61173058e+00 -1.45945826e+01  9.38081169e+00  9.56013584e+00\n",
      "  9.12343502e+00 -5.35078125e+01  1.01525145e+01  9.18075657e+00\n",
      "  7.07097960e+00  9.26743698e+00  8.21152306e+00  9.36077023e+00\n",
      " -1.22065220e+01  8.79615688e+00  9.08653641e+00  7.98202801e+00\n",
      " -7.54374599e+00 -5.02437668e+01  8.16289997e+00 -2.96830177e+01\n",
      "  4.61986113e+00  4.81244946e+00  9.98855782e+00  9.43576813e+00\n",
      "  9.65639591e+00 -4.74477348e+01 -6.28655577e+00  9.89722919e+00\n",
      "  9.87845898e+00  9.86254978e+00  6.80836010e+00 -4.38375053e+01\n",
      "  7.89344978e+00  8.94587040e+00  9.52573204e+00  1.54103267e+00\n",
      " -9.84739590e+00  9.38046551e+00  9.03585434e+00 -6.96632080e+01\n",
      " -6.92327423e+01 -2.77615700e+01 -5.96913147e+00  9.53389931e+00\n",
      "  9.11343288e+00 -7.04840012e+01  6.73321009e+00  8.92968464e+00\n",
      "  4.92266273e+00  8.75652313e+00 -6.34070473e+01 -7.05553741e+01\n",
      " -6.04236908e+01 -2.57902985e+01 -1.05967293e+01 -5.07428074e+00\n",
      "  5.30935955e+00  5.22151470e+00  9.74190617e+00 -3.00874138e+01\n",
      "  8.90389156e+00 -5.62607841e+01  9.39664936e+00  8.91576576e+00\n",
      "  4.65325356e+00  8.09893322e+00  1.63811147e+00 -5.22351875e+01\n",
      "  9.99159813e+00  9.69531441e+00  9.98381996e+00 -1.05290012e+01\n",
      " -5.67616081e+01 -6.03274803e+01 -4.43472328e+01  9.44820023e+00\n",
      " -5.03030357e+01  4.41656709e-01 -1.87247276e+01 -2.39716072e+01\n",
      " -3.69783592e+01 -5.32940140e+01 -3.87519608e+01 -6.48694038e+00\n",
      "  1.07226591e+01 -6.88030396e+01  1.88229263e+00 -3.57189674e+01\n",
      "  9.95310116e+00  9.55416965e+00 -4.43874321e+01 -1.29817743e+01\n",
      "  8.64865780e+00 -6.11070518e+01 -1.24445553e+01  9.51618481e+00\n",
      "  9.01951218e+00  9.35424232e+00  2.37260270e+00 -6.68354263e+01\n",
      " -7.13508530e+01  9.61741924e+00  2.04462647e+00 -6.92754974e+01\n",
      "  9.52896309e+00 -1.53849335e+01 -9.36079121e+00  1.01429720e+01\n",
      " -6.56330566e+01 -7.06414490e+01  7.50727463e+00 -6.85856705e+01\n",
      " -2.59356995e+01  8.94649088e-01  7.22167492e+00  4.54084396e+00\n",
      "  6.85838223e+00  5.55762339e+00  9.41887856e+00  1.26549244e+00\n",
      "  9.37019730e+00  7.99805737e+00 -5.80885849e+01  5.47066784e+00\n",
      "  8.64581013e+00  9.56522655e+00  9.84835339e+00  1.92336071e+00\n",
      " -8.61032546e-01 -6.60535431e+01  7.75569737e-01  9.71490479e+00\n",
      "  9.61109638e+00  1.04457235e+01 -1.50155985e+00 -4.00466270e+01\n",
      "  9.57130909e+00  3.99256682e+00  7.44127274e+00 -6.48150940e+01\n",
      "  9.98635101e+00  8.17375183e+00  9.53087330e+00 -4.57187355e-01\n",
      " -1.03013191e+01  9.29514027e+00  5.41082954e+00 -2.97103577e+01\n",
      " -2.77930045e+00 -6.81293488e+01  9.64099407e+00 -9.56005192e+00\n",
      " -9.27962017e+00  6.23851204e+00  9.09509945e+00 -3.13219681e+01\n",
      "  9.69908428e+00  3.44348192e-01  1.09659548e+01 -2.14500427e+01\n",
      "  6.71611309e+00  9.77807713e+00  8.88974571e+00 -9.30701828e+00\n",
      "  9.68558407e+00 -9.52012539e+00  5.46268940e+00  8.77640629e+00\n",
      " -7.12204285e+01 -3.32107735e+01  9.64250660e+00 -2.41280913e+00\n",
      "  5.95486212e+00 -1.95337620e+01 -7.07915573e+01 -7.00360565e+01\n",
      " -7.01981049e+01  8.90424252e+00 -5.59876671e+01  8.24120712e+00\n",
      " -1.07709217e+01  6.65686512e+00  9.20668221e+00  1.05520067e+01\n",
      " -1.03388662e+01  3.57874185e-02 -5.25496674e+01 -6.05659027e+01\n",
      "  9.41786098e+00 -2.46500206e+01  3.64522552e+00 -3.47978363e+01\n",
      " -3.37778282e+01  8.88173294e+00  8.77947807e+00  1.00315485e+01\n",
      "  1.01399899e+01 -4.74421883e+00 -2.56444302e+01  9.87445164e+00\n",
      "  2.37995076e+00 -3.56199150e+01 -6.28830528e+01 -6.69747620e+01\n",
      " -5.95224495e+01  1.01679087e+01 -1.57106581e+01  9.45212078e+00\n",
      "  2.52200484e+00  7.78006649e+00  7.70549774e+00  4.40659344e-01\n",
      " -6.63810806e+01 -4.46002083e+01 -3.25255585e+01 -4.11442261e+01\n",
      " -3.75250053e+01  9.27598286e+00 -4.86829090e+00  9.71744823e+00\n",
      "  1.00068388e+01  9.22072220e+00 -5.96734881e-01 -6.78211365e+01\n",
      "  7.17268372e+00 -1.88528442e+01  9.24716282e+00 -6.12766533e+01\n",
      " -5.10613680e+00 -5.71450539e+01  2.55253792e+00  7.27971983e+00\n",
      "  5.91812468e+00 -2.99261932e+01 -4.74117935e-01 -6.06388435e+01\n",
      " -7.05124283e+01  9.61817074e+00 -1.14186487e+01 -4.48277779e+01\n",
      " -5.29339561e+01 -2.31894150e+01 -2.72969189e+01 -3.00961018e+01\n",
      "  9.56437778e+00  8.95374203e+00  9.96579456e+00 -4.49658813e+01\n",
      "  8.88282204e+00  5.57966042e+00 -5.92034874e+01  9.06529963e-01\n",
      " -3.43142815e+01 -6.01617050e+00 -5.48069954e+01  9.56869602e+00\n",
      "  9.58142948e+00 -5.26557693e+01  2.18747950e+00 -3.47376175e+01\n",
      " -1.83615220e+00  9.69429111e+00 -1.70655899e+01  5.81366968e+00\n",
      " -6.11047363e+01  9.45230770e+00  9.53672409e+00  3.51124358e+00\n",
      "  7.96909714e+00 -6.77356110e+01 -1.71266210e+00 -3.64461746e+01\n",
      " -6.38033714e+01  7.81048679e+00 -3.32254295e+01 -1.93142681e+01\n",
      "  9.44603634e+00  1.18163300e+01  7.17234802e+00  3.70399308e+00\n",
      "  9.35769176e+00  8.79586697e+00  9.68556881e+00  7.91552067e+00\n",
      "  9.51597118e+00  1.68035305e+00  9.98199177e+00 -1.66266556e+01\n",
      "  3.05186570e-01 -7.11377487e+01 -7.04277649e+01 -2.94629784e+01\n",
      " -4.79556503e+01  9.85844517e+00  1.10179958e+01 -7.18197174e+01\n",
      "  9.83113480e+00  8.68781281e+00  9.71386719e+00  7.32249546e+00\n",
      "  9.51009846e+00 -6.08588696e+00 -6.09847908e+01  4.45756388e+00\n",
      "  8.51835155e+00  1.04289780e+01  9.36340332e+00 -3.04539859e-01\n",
      " -5.33422127e+01 -6.17167625e+01  8.97615433e+00  9.47829533e+00\n",
      "  9.57530499e+00  9.51834106e+00 -1.57545481e+01 -4.37389469e+00\n",
      " -6.41175308e+01 -2.22064552e+01 -5.86590462e+01 -4.18408871e+00\n",
      " -2.71041794e+01  1.04884977e+01  9.71701145e+00  8.18750763e+00\n",
      "  9.09038162e+00 -3.57877998e+01  9.00993919e+00  8.93257904e+00\n",
      "  9.18473339e+00  8.25990295e+00  6.06765652e+00  7.11629200e+00\n",
      "  9.93462181e+00 -1.43279085e+01  6.18840599e+00  8.89728260e+00\n",
      "  1.05687294e+01  2.01227382e-01 -5.93664050e-02 -2.56279526e+01\n",
      "  1.01220407e+01  8.91656113e+00 -3.79163628e+01  9.05200195e+00\n",
      "  3.62394166e+00  9.88942814e+00 -4.35394783e+01 -9.30803490e+00\n",
      "  2.01213196e-01 -6.95607605e+01 -6.62298126e+01  2.98956418e+00\n",
      "  9.37805271e+00  9.42592239e+00  8.57675838e+00 -7.05743103e+01\n",
      "  1.00647058e+01 -1.35123072e+01  7.69378006e-01  9.67799759e+00\n",
      " -4.74514122e+01  9.83267021e+00  9.43299484e+00  8.00698185e+00\n",
      "  1.72139490e+00  6.34960175e+00 -3.64118233e+01  9.31846905e+00\n",
      "  6.35193443e+00  1.05927601e+01  1.07569494e+01  9.37194443e+00\n",
      " -3.20905228e+01 -1.11847687e+01  9.07927513e+00  5.36205387e+00\n",
      " -2.14732285e+01  9.54632664e+00 -3.19475479e+01  1.00853348e+01\n",
      " -4.40494156e+01 -7.08792267e+01  1.14448433e+01 -3.81481781e+01\n",
      "  1.01802340e+01  9.85005569e+00 -7.00222015e+01  1.81168330e+00\n",
      " -5.76589346e+00 -5.63185921e+01 -4.99651451e+01 -1.71441853e+00\n",
      " -7.05808716e+01  1.05897236e+01 -2.96683273e+01  9.78962612e+00\n",
      " -6.58496284e+00  5.66263437e+00  6.11365795e+00  9.72556973e+00\n",
      " -6.71855621e+01 -1.04021025e+01 -2.06129036e+01 -6.35299492e+01\n",
      " -2.45317039e+01 -5.13043137e+01 -5.15326042e+01  8.89181042e+00\n",
      " -3.05907021e+01  7.71923208e+00  6.87262201e+00 -2.18994560e+01\n",
      " -1.65543156e+01 -3.06137772e+01 -6.35785637e+01  6.93090439e+00\n",
      " -4.61953926e+00  9.72914600e+00  9.95880985e+00  1.87300229e+00\n",
      "  8.43778610e+00 -1.21914988e+01 -6.93512192e+01  4.66832542e+00\n",
      "  9.86490154e+00  9.33470631e+00 -3.89923515e+01  9.86024380e+00\n",
      "  9.57837772e+00  9.87354469e+00 -5.76260948e+00  9.84631443e+00\n",
      " -2.21423607e+01 -6.49381638e+01 -7.11014099e+01 -5.43781662e+01\n",
      "  9.39510918e+00 -4.67094460e+01  9.70426369e+00 -1.33914738e+01\n",
      " -6.15960197e+01  5.20193481e+00 -4.47439194e+00  5.99720001e+00\n",
      "  9.94816208e+00  9.97741890e+00  8.98582554e+00  9.96583176e+00\n",
      " -6.67966309e+01 -6.42884979e+01  9.80102634e+00  9.60240078e+00\n",
      "  9.63761997e+00 -3.61142588e+00 -6.49781189e+01 -6.88956146e+01\n",
      " -1.18719387e+01  9.72530174e+00  5.09066010e+00  9.52786541e+00\n",
      "  8.99041367e+00  9.42915916e+00 -2.16289940e+01 -2.30781326e+01\n",
      " -2.25125732e+01  1.65502059e+00 -4.48239040e+00 -3.14118061e+01\n",
      "  9.15878773e+00 -2.29279785e+01  8.04330730e+00 -2.03673363e+01\n",
      " -1.42511768e+01 -5.18591583e-01  9.61380196e+00  9.73324203e+00\n",
      "  9.66803265e+00 -1.02084665e+01  9.74093246e+00 -7.04011459e+01\n",
      " -4.83755989e+01  9.64345932e+00  6.54093218e+00 -2.63863335e+01\n",
      " -7.10588760e+01  1.02148733e+01  9.68823528e+00 -6.81416779e+01\n",
      "  2.53354049e+00  9.58547783e+00  9.90076542e+00  5.91066170e+00\n",
      " -3.32256079e+00  9.66921902e+00 -2.28968086e+01  9.36439812e-01\n",
      " -1.99702263e+01 -2.35309863e+00  1.03019209e+01 -2.27620721e+00\n",
      " -1.71643314e+01  9.56277466e+00 -3.01242733e+01  9.38368988e+00\n",
      " -6.78613510e+01  9.33314419e+00  9.17920208e+00  8.87324238e+00\n",
      " -1.70123711e+01  3.21507502e+00 -7.00794525e+01  9.39719391e+00\n",
      "  3.13409472e+00 -7.05095520e+01  9.71938038e+00 -1.54050195e+00\n",
      " -2.62182045e+01  5.49647236e+00  3.94668841e+00 -6.88594894e+01]\n",
      "Predictions for Y:  [ 9.36616611e+00 -3.83658562e+01  5.70140791e+00  9.87152672e+00\n",
      "  7.64396715e+00  8.95512295e+00  8.34646893e+00  9.39983273e+00\n",
      "  9.74520779e+00  9.18844032e+00  9.79455566e+00 -4.73475380e+01\n",
      " -6.43714600e+01 -5.21376724e+01 -5.28052711e+00 -1.56540794e+01\n",
      " -6.77775955e+01 -3.06745014e+01  6.83782911e+00 -5.45518227e+01\n",
      " -6.05385475e+01 -2.65071011e+01  8.61270142e+00 -5.91023088e-01\n",
      " -6.43626633e+01 -5.73098660e+00 -5.49198380e+01 -6.70037308e+01\n",
      " -4.70570278e+00  8.90936470e+00 -5.73555040e+00  9.74506092e+00\n",
      "  9.91961956e+00 -6.98627930e+01 -1.22876348e+01 -6.61721573e+01\n",
      " -6.21334686e+01 -2.16681442e+01 -7.08368988e+01 -6.04637909e+01\n",
      "  5.53209496e+00  9.49053955e+00 -6.90940399e+01  1.00066080e+01\n",
      " -6.07052689e+01  1.01571760e+01 -6.97688065e+01  9.92827034e+00\n",
      "  9.91407108e+00 -5.18854866e+01  2.69977379e+00 -3.39834023e+01\n",
      " -2.16381969e+01  5.53480339e+00  9.58312702e+00  9.74668980e+00\n",
      " -3.63786507e+01  6.60398841e-01 -6.74339828e+01  8.71730995e+00\n",
      " -6.29256325e+01 -6.55084381e+01 -3.63836441e+01  6.95541525e+00\n",
      " -3.54128151e+01 -4.48867073e+01 -6.55886765e+01 -4.13447151e+01\n",
      "  2.74298191e+00  9.21683693e+00  7.27052832e+00 -1.28907862e+01\n",
      " -4.89043922e+01  9.91755772e+00  9.55687714e+00  6.54844141e+00\n",
      " -5.87259388e+00 -5.19735680e+01 -7.82147551e+00 -2.13537827e+01\n",
      "  6.71434641e+00 -1.34212885e+01  4.87025928e+00  8.74587727e+00\n",
      "  8.68097973e+00  4.00916767e+00 -4.22730637e+01  9.90753841e+00\n",
      " -3.18890905e+00 -6.03256645e+01  1.01296806e+01 -4.68450508e+01\n",
      " -1.08472891e+01  8.20153046e+00  9.31036663e+00  9.96906662e+00\n",
      " -6.74967957e+01 -6.98215342e+00  5.69809103e+00 -7.16552658e+01\n",
      "  9.98985863e+00 -1.58967257e+01  3.78098297e+00 -6.84397964e+01\n",
      " -7.14277802e+01 -2.90885944e+01  6.92091990e+00 -2.60446281e+01\n",
      " -3.98392701e+00  9.23448372e+00  4.99656200e+00 -1.26884594e+01\n",
      "  6.84386826e+00 -5.16588807e-01  9.92441654e+00 -1.27304153e+01\n",
      "  9.02200031e+00  7.64831066e-01 -4.97499809e+01  1.00283659e+00\n",
      "  8.02810192e+00 -4.28825264e+01  1.00888224e+01  5.84199715e+00\n",
      " -7.10212936e+01 -6.61394958e+01  1.01137724e+01  3.22282314e+00\n",
      " -6.17918243e+01  3.46848607e+00 -1.47742481e+01 -4.18849182e+00\n",
      "  9.97515583e+00  9.85663700e+00 -4.93281212e+01  9.72432423e+00\n",
      "  8.74607182e+00 -5.41990423e+00  7.90523958e+00 -6.64973984e+01\n",
      "  8.19703770e+00  9.60049725e+00 -8.87135887e+00 -3.33112717e+00\n",
      " -5.36974449e+01 -1.27289753e+01  8.84782505e+00  9.57263279e+00\n",
      "  9.59408474e+00 -3.02006507e+00 -8.13768673e+00  7.24793482e+00\n",
      "  1.01383772e+01  9.78103352e+00 -6.07568245e+01 -1.50783253e+01\n",
      "  9.80737400e+00  1.00250254e+01  9.74929047e+00 -1.28432560e+01\n",
      " -6.88556519e+01 -6.34384117e+01  6.85592747e+00  1.01453428e+01\n",
      " -5.05456886e+01 -5.75969658e+01  8.31950951e+00  6.99030399e+00\n",
      "  3.68227005e+00 -3.43600616e+01  8.67874527e+00 -7.05488920e+00\n",
      "  9.59607029e+00 -3.26822777e+01 -5.86869240e+01  9.94079876e+00\n",
      " -2.04179668e+01 -3.97374487e+00  3.60727453e+00 -3.91097679e+01\n",
      " -5.54894180e+01 -2.52996998e+01 -1.21885252e+01  9.20047665e+00\n",
      " -5.38181801e+01 -6.47578430e+01 -2.89543858e+01 -6.85678635e+01\n",
      " -6.51741257e+01 -1.53661432e+01 -4.15919991e+01 -3.79128914e+01\n",
      " -4.73126831e+01 -8.28288496e-02 -5.38441124e+01  1.79226768e+00\n",
      "  5.03428221e+00 -5.45987463e+00  9.37041759e+00  9.97158718e+00\n",
      " -4.71641960e+01  1.00792809e+01 -4.07714033e+00  9.31580734e+00\n",
      " -5.42024422e+01  9.93475151e+00 -1.89380550e+01  9.35014153e+00\n",
      "  9.51411057e+00 -6.19994736e+00 -2.85052395e+01 -7.01698074e+01\n",
      "  7.07264328e+00 -6.61513138e+01  9.30256653e+00 -6.75078278e+01\n",
      " -4.50403633e+01  9.37557030e+00  7.78064394e+00 -9.12620163e+00\n",
      "  9.08477306e+00  9.85431576e+00 -1.00460684e+00 -4.28305626e+01\n",
      " -3.74471992e-01  4.61890268e+00  9.61766815e+00  1.92753470e+00\n",
      "  8.91804409e+00  8.25245094e+00  9.49164772e+00  9.51861477e+00\n",
      "  9.49828053e+00 -6.30640755e+01 -1.75272083e+01 -4.88751411e+00\n",
      " -5.35115166e+01  9.90588665e+00  9.42171669e+00 -8.80638242e-01\n",
      " -1.43715870e+00  9.82341766e+00 -3.22629852e+01  7.76734591e+00\n",
      "  4.93930101e+00 -4.91274681e+01 -3.62484856e+01  1.06078017e+00\n",
      "  7.98353148e+00 -2.13806973e+01  9.82599354e+00  9.61912060e+00\n",
      " -1.15430937e+01 -3.61568413e+01 -5.54490623e+01 -6.93148575e+01\n",
      " -4.97285347e+01  9.90256691e+00 -1.11622686e+01 -3.95630417e+01\n",
      "  9.61295605e+00 -6.97239914e+01 -1.05710828e+00  9.83156300e+00\n",
      "  8.70972252e+00 -8.82897091e+00 -3.61478157e+01  9.46675682e+00\n",
      " -3.30777779e+01  9.94419956e+00 -5.73196678e+01 -1.19999075e+01\n",
      " -2.70974941e+01 -1.11621256e+01  9.48904324e+00  9.52729607e+00\n",
      " -2.14481945e+01 -7.26482544e+01 -6.40083771e+01 -5.14908791e+01\n",
      " -1.29954920e+01  9.61042881e+00 -3.83354149e+01 -2.35215950e+00\n",
      "  9.91344738e+00  1.67365539e+00 -3.81044044e+01  8.24590969e+00\n",
      "  6.19874597e-01 -1.27110615e+01 -6.95460129e+01  9.12489700e+00\n",
      " -1.29717946e+00 -5.04013901e+01  9.06543636e+00  9.68699265e+00\n",
      " -2.17707214e+01 -1.14443274e+01 -2.63646240e+01  1.01293974e+01\n",
      "  1.01713324e+01 -5.96188927e+01  8.10358238e+00  8.81086445e+00\n",
      " -5.54452286e+01  9.90373993e+00 -3.00304241e+01 -5.19405317e+00\n",
      " -2.49049902e+00 -4.91554146e+01 -5.03848381e+01  9.88129807e+00\n",
      "  9.23285484e+00 -3.38528786e+01 -6.95802231e+01  9.63995171e+00\n",
      " -6.33213234e+01 -6.05101357e+01 -5.99401593e-01 -1.17545891e+01\n",
      "  9.90822601e+00  4.45242596e+00 -7.27216873e+01  4.04995829e-01\n",
      "  1.00003862e+01  9.06337643e+00  5.57067919e+00 -6.40317154e+01\n",
      " -3.00748196e+01 -6.96360703e+01  7.70757055e+00 -1.22670994e+01\n",
      "  9.82470512e+00 -5.59212036e+01 -2.57012844e+01 -2.15909538e+01\n",
      " -7.09603500e+01  9.14981747e+00  9.87930584e+00  9.85118198e+00\n",
      "  6.82634830e+00 -3.76535606e+01  8.26364040e+00 -2.03163185e+01\n",
      "  7.34109545e+00 -9.92871761e+00  9.52305031e+00  9.85869598e+00\n",
      " -1.43999834e+01 -5.12031059e+01  3.56771588e+00 -3.07979946e+01\n",
      " -9.16989422e+00  9.10897923e+00  1.77688825e+00 -3.34788742e+01\n",
      "  9.68304539e+00  2.07067400e-01  9.68096447e+00  8.17675018e+00\n",
      " -4.10610847e+01  9.87318516e+00 -4.39955559e+01  8.73709393e+00\n",
      " -2.39037037e+01 -2.89370480e+01 -3.21210365e+01 -5.64148293e+01\n",
      "  7.89623404e+00  9.77076435e+00  5.78469849e+00 -6.91585388e+01\n",
      "  9.98398018e+00  9.78473568e+00  1.24774367e-01 -3.77939377e+01\n",
      " -4.46627846e+01 -3.86010504e+00 -2.03330441e+01  9.91274643e+00\n",
      " -6.84533310e+01  9.84394264e+00  8.99730778e+00  9.07489586e+00\n",
      " -1.55649939e+01 -2.20167294e+01 -1.21654396e+01 -3.38268166e+01\n",
      "  9.96411133e+00 -3.76471100e+01  9.44173527e+00  9.44509792e+00\n",
      " -6.46852875e+01 -5.11259727e+01  9.92890358e+00  1.02630062e+01\n",
      "  1.00568018e+01  8.60936165e+00 -5.30810814e+01  9.83984852e+00\n",
      "  9.98431492e+00 -6.67683411e+01 -6.25361490e+00 -4.21757126e+01\n",
      "  8.17149067e+00  8.63610744e+00 -6.77290878e+01  4.38835430e+00\n",
      " -1.02213573e+01  4.50838757e+00  9.67026997e+00 -3.66252708e+01\n",
      "  1.02750998e+01 -5.61903915e+01  5.75642776e+00 -7.01752853e+01\n",
      " -6.82723389e+01 -1.42882357e+01  5.09238541e-02  4.94149828e+00\n",
      " -3.52479696e+00 -4.71198196e+01  9.58941174e+00  6.70585394e+00\n",
      " -1.55946951e+01 -1.13494170e+00  9.93715954e+00 -4.95327911e+01\n",
      " -6.71483688e+01  9.44585514e+00 -1.26958342e+01 -3.87708807e+00\n",
      " -1.48361406e+01 -4.92447510e+01 -6.48456192e+01  9.22155666e+00\n",
      "  9.75168133e+00  9.91218948e+00  7.11468172e+00  9.83543587e+00\n",
      " -2.29506874e+01  6.36154509e+00 -4.39151802e+01 -3.35878830e+01\n",
      "  9.98862076e+00 -5.35376778e+01 -7.23955765e+01 -2.58280354e+01\n",
      "  3.04942656e+00 -1.26058826e+01 -6.35760927e+00 -6.62509842e+01\n",
      " -2.22006359e+01 -1.48302279e+01  9.15174389e+00  9.39102650e+00\n",
      "  8.19061375e+00  9.90870571e+00 -5.81764565e+01  9.93555737e+00\n",
      "  9.42920017e+00 -2.17577534e+01 -4.10327682e+01  9.43137646e+00\n",
      " -1.17389603e+01 -4.12016821e+00 -7.01311493e+01  9.15209675e+00\n",
      "  3.33933210e+00  9.08274746e+00 -3.40832558e+01 -5.92215919e+01\n",
      "  9.30112839e+00  1.07889261e+01 -6.44598999e+01  7.28764677e+00\n",
      " -1.66213531e+01 -2.98952246e+00 -7.08187790e+01  6.07907772e+00\n",
      " -2.47700739e+00  6.92658186e+00  9.79110813e+00 -3.42371345e+00\n",
      " -3.13255672e+01  9.85865593e+00  9.61895084e+00 -1.84112728e+00\n",
      " -7.99837971e+00  3.15054131e+00 -5.72888298e+01 -2.30245233e+00\n",
      " -4.89615898e+01  9.17393017e+00  9.61040211e+00  9.98420429e+00\n",
      "  9.90802860e+00 -6.57009659e+01 -4.04547424e+01  9.87953091e+00\n",
      "  8.48159599e+00  1.01283159e+01  7.76320314e+00  9.91791439e+00\n",
      " -1.84528904e+01 -9.14864182e-01  2.00530100e+00 -7.08721237e+01\n",
      "  9.82137775e+00 -7.73716211e+00 -7.20595169e+01  8.26279354e+00\n",
      "  8.78180218e+00 -9.67928791e+00 -7.21663284e+01 -3.38312035e+01\n",
      "  6.26339293e+00  9.92196274e+00  9.89186668e+00 -1.05642242e+01\n",
      " -4.35212860e+01 -5.00911407e+01  9.12458777e-01 -7.12625656e+01\n",
      " -6.72603455e+01 -4.58828773e+01 -6.41237183e+01 -4.59256783e+01\n",
      " -7.15440674e+01 -5.13136292e+00  7.46781158e+00 -6.61001205e+01\n",
      "  9.30270767e+00 -6.32256279e+01  9.23552608e+00 -4.27387695e+01\n",
      "  5.28722143e+00  9.01829910e+00 -1.32595711e+01  9.64700031e+00\n",
      " -6.12401161e+01  5.79394674e+00 -2.63919010e+01 -6.35455284e+01\n",
      "  7.58406496e+00  9.89683151e+00 -6.58455887e+01  9.56523228e+00\n",
      " -2.77821732e+00  9.69745827e+00 -4.45847168e+01  9.95409012e+00\n",
      " -6.44853134e+01  1.00215025e+01 -3.51294994e+00 -1.70781791e+00\n",
      "  2.16495824e+00 -2.27899246e+01  8.28309441e+00  1.00742445e+01\n",
      " -6.47568741e+01  9.66289997e+00 -1.76124897e+01 -1.72669923e+00\n",
      " -6.74665146e+01 -7.12633362e+01  7.13996792e+00  7.27278137e+00\n",
      " -4.40040932e+01  9.96864414e+00  9.81865978e+00  9.43506336e+00\n",
      " -4.13846474e+01 -3.58917503e+01  9.25857639e+00 -4.97442532e+00\n",
      " -7.53687143e+00 -6.96955109e+01  1.04299850e+01  5.12081051e+00\n",
      " -7.08091507e+01  9.19882393e+00 -1.45084848e+01  6.02352619e+00\n",
      " -1.81775417e+01 -4.27474165e+00 -6.28547707e+01  8.63605404e+00\n",
      "  9.78346062e+00 -5.57908363e+01  1.02691040e+01  9.82785511e+00\n",
      " -7.73104906e+00 -3.37964401e+01  9.55156994e+00  8.94165134e+00\n",
      "  9.35996342e+00 -2.73309851e+00  9.60071945e+00 -1.24173775e+01\n",
      " -5.32646751e+00  9.82077026e+00  9.57389164e+00  4.41435194e+00\n",
      "  1.02464638e+01 -5.79700661e+01 -3.70753365e+01 -1.59000711e+01\n",
      "  8.76102161e+00 -1.80393753e+01  9.49437618e+00 -6.27841835e+01\n",
      " -6.45626068e+01 -4.62472267e+01 -7.22483139e+01 -1.45309649e+01\n",
      " -1.04296100e+00 -1.37313147e+01 -6.48690567e+01 -3.33237381e+01\n",
      " -2.56095943e+01 -5.25056038e+01  9.68259907e+00 -4.37769814e+01\n",
      " -7.24371624e+00  9.16698742e+00 -1.31713600e+01  7.51714945e+00\n",
      "  9.49746037e+00 -6.35184898e+01 -1.77007008e+01  9.83956051e+00\n",
      "  9.68520641e+00  9.59245586e+00 -2.06037464e+01 -7.16917515e+00\n",
      "  8.17666340e+00 -2.55086250e+01  9.83615875e+00  9.51233196e+00\n",
      "  9.41467381e+00  7.54277468e+00 -6.11019173e+01 -6.56094894e+01\n",
      "  7.42958927e+00 -3.41973648e+01  6.06866169e+00 -6.97119980e+01\n",
      "  1.00328035e+01 -3.83779001e+00  9.97498226e+00  8.88687611e+00\n",
      " -1.28350582e+01 -6.00933952e+01 -4.33153572e+01  9.82743549e+00\n",
      "  9.39815140e+00 -3.53134613e+01  9.63042927e+00 -6.89776382e+01\n",
      " -2.98723173e+00 -5.11249428e+01 -1.84052906e+01  1.02789812e+01\n",
      "  7.21597147e+00 -7.07852325e+01 -6.05705738e+00  1.00015163e+01\n",
      " -7.12358856e+01  2.75322533e+00  4.86134678e-01  9.92991543e+00\n",
      "  8.73930550e+00  5.36006165e+00 -1.50333014e+01 -1.05493963e+00\n",
      " -1.83653793e+01 -3.84240189e+01 -1.93575039e+01  9.60691357e+00\n",
      "  7.17660666e+00  9.36492729e+00  7.12218285e+00 -6.79248505e+01\n",
      " -5.31627083e+01 -3.77034515e-01  8.53266430e+00 -7.07978592e+01\n",
      "  7.60156393e+00 -1.34655981e+01 -3.79879913e+01  7.83643675e+00\n",
      " -3.17317724e+00 -6.64159656e-03 -6.24692345e+01  9.76775646e+00\n",
      " -1.06013889e+01  9.72658348e+00 -1.65611439e+01 -7.07673111e+01\n",
      "  9.67626095e+00 -8.32318592e+00  7.43202448e+00  9.77154350e+00\n",
      "  9.04519272e+00  9.14510822e+00 -4.87831535e+01 -4.94275703e+01\n",
      "  9.61002445e+00 -5.20689354e+01  1.01546774e+01 -5.72791672e+01\n",
      " -2.45879154e+01  9.81672668e+00 -6.46898880e+01 -3.11311321e+01\n",
      " -1.10142078e+01  8.97884655e+00  9.63821697e+00 -4.92786713e+01\n",
      " -1.72527707e+00 -3.39310455e+01 -3.56249657e+01 -5.56452255e+01\n",
      "  9.58806324e+00  4.77346945e+00 -3.70380898e+01 -6.63754196e+01\n",
      "  9.85201168e+00 -2.72073555e+00 -7.57520056e+00 -7.02760620e+01\n",
      "  4.09997606e+00  8.29129314e+00 -5.28421745e+01  3.66278481e+00\n",
      " -1.11869371e+00 -5.85636902e+01 -7.13250427e+01  8.11808300e+00\n",
      "  8.76879787e+00 -2.47633495e+01 -4.41488304e+01 -4.77736816e+01\n",
      " -7.06581421e+01 -6.27102165e+01  9.06023216e+00  9.10187435e+00\n",
      "  9.69773293e+00 -2.83963442e+00 -5.95301294e+00  8.48158169e+00\n",
      "  9.61422634e+00 -6.56527939e+01  9.04916763e+00 -6.43174744e+01\n",
      "  1.01574192e+01 -3.52237314e-01 -4.20554237e+01 -7.24019670e+00\n",
      " -2.97791500e+01  1.49026668e+00 -3.32071533e+01  8.77616024e+00\n",
      " -2.47909975e+00  1.89198911e+00  4.30201387e+00  9.86363125e+00\n",
      " -6.91628571e+01 -2.28317719e+01 -2.80430813e+01 -7.26617813e+01\n",
      " -8.77242208e-01 -2.83935261e+00  9.89223194e+00  6.04063368e+00\n",
      "  9.45687199e+00 -6.54371796e+01 -6.37480850e+01  7.02299690e+00\n",
      " -6.90404272e+00  9.93200207e+00  1.03155947e+01  1.04849072e+01\n",
      "  1.06051102e+01 -2.36105461e+01 -2.64413204e+01 -2.90960484e+01\n",
      "  9.63801765e+00  1.01725407e+01  9.64665318e+00 -5.43874893e+01\n",
      " -5.95084419e+01 -4.51642570e+01 -1.24219828e+01 -5.84986153e+01\n",
      "  9.19092655e+00  6.41272449e+00 -1.03917074e+01  9.40148067e+00\n",
      " -5.05426216e+01 -6.44743347e+01 -3.78386593e+00 -6.78095322e+01\n",
      " -2.58128452e+01 -1.66240501e+01 -3.48723450e+01  2.84177017e+00\n",
      " -3.07092342e+01  4.34445906e+00  9.88561249e+00 -1.84927711e+01\n",
      "  9.93340397e+00  4.82746696e+00  2.87552261e+00 -2.37296352e+01\n",
      "  4.27925348e+00 -3.24248772e+01  5.62589359e+00  9.51960278e+00\n",
      "  1.08191977e+01 -6.70518799e+01  9.77073002e+00 -6.43607941e+01\n",
      "  1.68523002e+00  1.43912351e+00  9.31690979e+00 -1.01047716e+01\n",
      "  2.62457466e+00  5.12601089e+00  5.89362383e+00  8.98611259e+00\n",
      "  8.29216480e+00 -7.03605728e+01 -8.41074753e+00 -1.72780126e-01\n",
      "  9.60775757e+00 -1.62637520e+00 -6.68028107e+01  7.08888197e+00\n",
      " -3.50827599e+01 -3.32869568e+01  9.94132519e+00  9.76021290e+00\n",
      "  1.05182962e+01 -7.15221939e+01  9.19851780e+00  9.48210239e+00\n",
      "  9.16224003e+00 -5.33956528e+01  1.02584047e+01 -5.58824348e+01\n",
      "  9.54683590e+00 -6.98054276e+01  7.83219957e+00 -4.65594578e+00\n",
      "  8.49131298e+00  9.35053349e+00  6.58691740e+00  1.05831945e+00\n",
      "  8.72581005e+00  8.73494434e+00  9.66841602e+00  9.96835613e+00\n",
      "  2.98978901e+00 -3.58087492e+00  8.21089363e+00 -5.75084925e+00\n",
      " -7.06146317e+01  1.42049938e-01  3.89568663e+00 -6.46714783e+01\n",
      " -6.85229416e+01  4.67012310e+00 -5.34487190e+01  9.85501003e+00\n",
      " -5.64447365e+01  9.94546032e+00 -9.86367893e+00  9.68772125e+00\n",
      "  1.00605726e+01 -5.73077679e-01  9.54293919e+00 -3.47945557e+01\n",
      " -3.57750607e+00 -3.58415184e+01 -1.55843382e+01 -2.07749100e+01\n",
      " -6.11547661e+01  9.94070339e+00 -5.54869056e-01 -3.60422745e+01\n",
      "  9.81681347e+00 -3.93087506e+00 -1.12739744e+01  9.97942066e+00\n",
      "  9.77631378e+00  9.91633320e+00 -6.97206955e+01  6.23797846e+00]\n",
      "Predictions for Z:  [19.955904   34.88526    20.14026    24.818436   30.253859   13.576383\n",
      " 13.707234    6.4152284   6.2330604  23.640602   12.8981695  14.899035\n",
      " 16.666159    9.4564295   5.1366677   7.1372714   8.638134   33.97544\n",
      " 13.945891   15.2486515   2.9080095  20.912294   23.466738   16.928745\n",
      "  3.1589057   7.788164    2.369259   28.336132    7.774548   26.214226\n",
      " 20.964539    6.219173    0.4237785  16.71417    32.052483   28.206219\n",
      " 35.23136    17.653666   34.92707     9.456851   11.640716   18.401926\n",
      "  9.768024   19.478716   15.490009   32.40109    10.693154   12.333842\n",
      "  0.3826117  27.89583    23.541067   17.079657    3.7494557  17.691599\n",
      " 18.415014    6.224915    8.59051     1.336447    9.398099   11.860362\n",
      " 35.306614    3.202841   21.743185    1.1994666   4.206463    9.527568\n",
      " 15.639169   22.505941    1.3584245  23.674858   17.822374   20.138031\n",
      "  9.438554   25.226889    6.370837   18.975573   26.788094    9.478461\n",
      " 14.701314    2.502326   17.813667    3.493231    7.9194903   6.7060237\n",
      " 19.989702   29.876171   16.290129    0.3872191  26.94911     4.264228\n",
      " 24.739285   20.542027    3.9502332   5.6644497  17.40132    25.3396\n",
      " 21.155401    1.2431514  17.748255   28.096912   32.393826    1.9296924\n",
      " 14.193133   27.982252   16.477972    2.706436    1.2012937  22.900467\n",
      "  5.009139    5.6701097  30.081156    7.5121307  13.945157    5.1021624\n",
      "  0.4449498   3.5663526   6.090881    8.04019    21.859211    1.3573292\n",
      " 23.80037    14.968028   18.398409   24.41131    16.407457   28.308052\n",
      " 18.486406    8.102069    3.0066187   1.3602208  20.027925   15.301694\n",
      " 19.734032   32.245537    9.385408   32.422604   23.455858    5.126085\n",
      "  5.7040887  35.363132    5.232615   13.075742    7.890613   22.599731\n",
      "  2.2525697   3.569275    6.668034   31.151228   10.645125   10.916271\n",
      " 23.053026   26.39279    32.399273    6.3258843  35.138035   33.26223\n",
      " 12.888599   19.610472    6.22203    10.871769   27.904173   16.666574\n",
      "  7.3183784  31.315086   28.4587     34.97284    11.800529    5.5854845\n",
      "  5.2930307  31.250189   30.499735   10.809527   12.40682    17.220324\n",
      "  9.4189415  12.384197   17.452852   32.975365    8.092102    9.541102\n",
      " 20.952524    7.901339   32.030273   11.753348   22.451777    3.1755366\n",
      "  4.152215   22.02888    22.402927   31.95051    14.970501   14.808717\n",
      " 21.951607   29.413092   20.836002    1.3707027   7.8780084  15.080901\n",
      " 13.392067   25.340807    9.088416   32.39478     5.029158    5.684633\n",
      " 20.860455   32.30347    10.797599   12.698716    5.4584613  10.654087\n",
      "  8.339717   10.22653     1.1919135   9.567924   13.428553    9.419863\n",
      "  3.8661475  17.376429   26.358664    7.8779535  19.974081   12.835788\n",
      " 29.26664    16.262169   29.354399   18.983072   12.400371   32.922867\n",
      "  0.9545567  23.516356   18.432688   12.194088    6.3839145  28.020683\n",
      " 32.28883     5.139188   22.338251   25.222406    6.4087157  14.291146\n",
      " 11.239951   12.2815275  10.262584    7.023091    5.350991    9.367717\n",
      " 21.745497   22.995508   23.92581    26.494638   18.389322    5.4749436\n",
      " 14.278476   21.746195   15.309972   16.076078   15.89388    25.303308\n",
      " 26.647665   21.522518   31.31923     9.904659   26.895247   12.882868\n",
      " 26.2485     26.761553    4.1541557   5.5670886  23.177113   19.324965\n",
      "  9.876729   26.64913    22.906763   31.783882   18.573198   17.505625\n",
      "  7.4512153  27.9319     35.406933    9.844398    7.498089   12.226653\n",
      " 30.85288    29.375875    0.38688934 23.204006   31.001284   23.515993\n",
      " 32.872242    1.6983149  20.464476   19.973104   29.253489   14.911816\n",
      " 13.540956   18.949112   20.41173    31.85164    33.510593   32.398155\n",
      " 25.491304    2.8210366  32.76824     6.0689054   9.103425   12.3550825\n",
      " 14.555607   26.884054   11.051388   21.873182   15.653781   25.367373\n",
      " 26.152796   21.697107   16.129452    0.622966   15.535054   15.9505825\n",
      " 20.425533   11.009063   25.998755   33.19681    27.91905    29.530283\n",
      " 25.010319    0.90829647  1.2772715  16.669367   17.428715   13.129622\n",
      " 17.648222    1.6635014  31.190065    2.466682   20.780972   33.65482\n",
      " 34.96671     6.533994   25.132584   24.765955   26.423285   31.201773\n",
      "  5.3485193   3.6405618  20.127916   26.687208    6.1257944  25.330858\n",
      "  7.2990413  35.397575   11.521827    2.5652056  31.403643    9.51767\n",
      " 11.50553    31.276121   12.273793    1.2908802   5.9823146  26.322222\n",
      "  9.80974    12.377057   29.014406   30.480705   32.01333    10.669426\n",
      " 34.36022    21.017593    6.9845157  32.41145    29.690554   16.141546\n",
      " 19.395586    6.325736   20.330828   31.153795   29.036867   20.934334\n",
      " 17.445139   12.325337    8.052819   31.343193    0.9294523  19.9733\n",
      "  1.8976783   3.7932847  20.234426   34.674465   19.369078   28.32482\n",
      "  6.4002004  32.478634   28.101728    9.8109665  25.985529   25.72376\n",
      " 18.39126    17.270021    2.1944165  23.817907   32.341488   21.126495\n",
      " 20.939459    8.67776     1.1050982  23.47236    35.204548   33.196003\n",
      " 33.187534   26.653473    6.34456    22.780264   25.705843   28.163443\n",
      "  7.6744556  10.120292   13.800391    1.7990385   1.2749381  11.60319\n",
      " 29.726986    9.310639   31.279875    7.371054    1.900625   22.501156\n",
      " 19.304064    9.556994    3.2402246  13.340594   32.13742    10.744752\n",
      " 10.908809    4.06476    16.66381    11.736411    6.274014    0.43045917\n",
      "  1.1903104  17.839926   14.093014   29.549576   21.11009    14.767198\n",
      " 32.344395    2.2385328  27.964838   22.894318   26.820028   33.085438\n",
      "  7.8085685  28.211552   22.460701    3.4319513  12.315564   19.95437\n",
      " 11.228177   25.302687   22.49053    18.281971   23.873592   10.797191\n",
      " 22.559742   11.809109   26.647844   22.640898   10.051904    4.9412174\n",
      " 26.80049    19.972132   31.243114   36.19675     6.455119   11.9681425\n",
      " 28.090242    7.1794353  13.714004    1.1477348  16.375677   24.4543\n",
      " 11.052813   13.931422    0.6629205  29.692438   27.90608    12.392052\n",
      " 31.308355   20.64964    10.991297   11.505813    2.5974019  16.10223\n",
      " 28.596579    6.09378    12.966699   32.33566     0.36863822  3.2084913\n",
      "  8.628221   25.310968    6.7971907  32.396      24.243788    0.4140951\n",
      "  2.2008176   8.012424   20.257425   34.932163    0.8755951  10.941449\n",
      " 16.645416   32.739983   19.983213   26.702253   27.943937   17.086876\n",
      " 14.035649   32.296997    0.41144922 31.649675    2.5247333  35.562023\n",
      " 14.156928    3.1463792  22.590841    2.1401322  35.416824   28.958332\n",
      " 17.193846   22.686209   13.8503475   3.5960824   5.6819844  15.530004\n",
      " 13.475398    9.859008    1.2959667   6.0899587  26.670687   12.2458105\n",
      "  2.9656682  20.14292    14.314583   35.748966   20.109762   12.178908\n",
      " 15.651967    0.6781069  10.981025   23.867723    2.3660872  25.314213\n",
      "  3.1641648  18.350702   10.818791   29.269226   29.933458   14.076684\n",
      "  5.621035   32.393536    9.700214   12.924577   20.135448   26.939816\n",
      " 22.398726    3.1838195  29.765276    5.6329837  21.095476   18.277418\n",
      " 25.392662    5.601788    3.7746487  34.713284    6.4787183  33.09188\n",
      "  1.2339352   3.1048763  31.464441   33.169693   16.37387     5.0502725\n",
      " 16.790731   24.445751   32.364567   20.969606   22.147734   13.650924\n",
      "  0.6199143   2.4540496  25.664837    0.7562195   4.6563134  31.258307\n",
      "  6.037207   23.492266   17.37884     1.1398215  31.304258    7.524072\n",
      " 33.132298   26.026423   12.153047   29.87983    25.57099    22.520052\n",
      " 28.338926   31.958517    5.7230186   3.5031168   6.1271777   9.200552\n",
      " 15.593751   22.109575   21.825333    3.431619   22.50823    33.10762\n",
      "  3.9451358  31.279696   31.743134    4.0170918  12.281308   16.269253\n",
      " 14.787049    6.5274005  20.105738    4.127277    0.716672   35.359425\n",
      " 23.144361    0.7302631  23.946157   31.279192    3.662617   14.796251\n",
      " 13.734513   22.87994    25.373606   17.480932   12.037472   20.11418\n",
      "  4.279015   28.286299   24.359987   28.395296   14.055755   34.99602\n",
      " 19.580185   15.391016   25.343897    3.757043   16.577026    9.5126\n",
      " 28.930387   12.442911   19.957165    1.8117125   6.358372   21.297491\n",
      "  1.1479217   9.810788    7.2051177  25.71189     7.201944    3.2767448\n",
      " 20.948109   31.2607      3.2033594  23.556093   29.55091    17.92835\n",
      " 17.17911    33.150944   26.653418   11.299609   33.720814   34.884464\n",
      "  7.293058    6.3626037   7.216581   11.770795    5.6079526  35.151447\n",
      " 15.131057   26.83919     6.781026   34.983936   10.5051565  10.803152\n",
      " 21.675825    7.0029216   7.792773    8.038768    4.2971954  18.443317\n",
      "  4.0059404   5.737345   11.084119   17.026842   12.277749   26.785116\n",
      " 32.882328    0.51713485  6.585179   30.664408    9.414748    9.539684\n",
      " 32.44238     2.0964668  32.419178   34.951485   27.113333   31.359804\n",
      " 28.260477   14.672479   26.649195    6.0887837  12.189673   28.569891\n",
      " 20.627842   21.702034    9.809414   28.171673   18.812668   33.18869\n",
      " 22.746103   16.044474   31.900686    4.8957095  15.172808   34.951393\n",
      "  8.051291    6.8567767  15.100984   23.754168   20.51546    36.190475\n",
      " 21.772139   26.32554    17.166151   33.562195   21.068752   35.79936\n",
      " 21.697573   15.513888   30.55705     0.8943435  12.916486   26.94642\n",
      " 26.776384    6.7929707  17.64326    28.169186    6.5800996  35.430305\n",
      " 24.629917    8.039765   16.306875   20.861437   10.588984   17.779627\n",
      "  1.931014   30.47558    20.749979   20.260199   11.562282   12.303807\n",
      " 27.914679   22.327711   22.897177   21.882494   22.520372   26.945704\n",
      " 17.898252   33.084587   11.819647    3.2009194  15.553365   32.94892\n",
      " 22.857946   12.253084   31.525537   11.985339   31.578955   10.693833\n",
      " 22.907639   14.461946    6.0226383  32.402565   12.936923   22.66212\n",
      "  9.496629    3.8782988  32.083763   21.15151    11.757959   26.448208\n",
      "  7.7370515   6.1170115   1.9846231   4.0660477   4.9652367   9.488341\n",
      " 33.52829    20.06785    17.043491   33.022152   27.853891   26.679783\n",
      " 18.373365   33.723206   18.259644    5.3448653  23.588902   32.056007\n",
      "  8.023521   23.151138    1.273656    5.444263   12.003966   28.337988\n",
      " 18.43498     3.1588714  32.908676   32.895355   13.427575    1.4403632\n",
      "  5.252282   30.099148   33.099285   26.198786    1.0891052   3.0686514\n",
      " 11.0540495   5.1360035  31.456186   14.352729    3.234534    7.2436376\n",
      " 34.711506   28.182915   18.25189     5.7820563  31.643446   16.495981\n",
      " 11.767966   31.20432    13.504189    9.275378   25.7377     16.04673\n",
      "  5.4240193  10.661137   13.786219    7.7733703  26.281689    5.683561\n",
      " 33.016987   29.700674   17.19201    26.244896   12.920214   24.025244\n",
      " 23.619963   20.90441    26.317207   30.542456   34.927036   14.195365\n",
      " 26.745779    3.1724112  11.183       1.3339673   9.269975   31.97589\n",
      " 16.031216   25.317404   20.499521   18.54036    24.745337    8.033936\n",
      " 31.15659     8.605357   15.49936    28.447302   13.737597   13.939885\n",
      " 22.09593    12.4359045  16.945047    8.592944   25.41312    10.733802\n",
      " 14.32244    25.344505    5.8857856   0.41191936 28.352926   11.677203  ]\n",
      "keras end\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\ngrid search params:     param_grid = {\\n        'n_units': [30],\\n        'init_mode': ['glorot_normal', 'glorot_uniform'],\\n        'activation': ['tanh', 'sigmoid'],\\n        'lmb': [0.0001, 0.001, 0.01],\\n        'eta': [0.001, 0.002, 0.003],\\n        'alpha': [0.5, 0.6, 0.7, 0.8, 0.9]\\n    }\\n\\n    \\nBest Parameters: {'alpha': 0.8, 'batch_size': 100, 'eta': 0.004, 'init_mode': 'glorot_normal', 'lmb': 0.005, 'n_units': 25}\\nBest Validation Loss: 2.98101544380188\\nBest Euclidean Distance Score: 1.5518332381772344\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_nn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
