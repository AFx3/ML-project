{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from keras import backend as K\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import time\n",
    "from sklearn.metrics import make_scorer\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "import csv\n",
    "from sklearn.model_selection import ParameterGrid, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load training data from CSV, excluding 1st and last 3 cols\n",
    "def read_tr(file_path, test_size=0.15, random_state=42):\n",
    "    # load tr data\n",
    "    train = loadtxt(file_path, delimiter=',', usecols=range(1, 14), dtype=np.float64)\n",
    "\n",
    "\n",
    "    x = train[:, :-3]  # Exclude 1st col and keep the last 3 as targets\n",
    "    y = train[:, -3:]  # targets\n",
    "\n",
    "    # Split the dataset \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# read TS data\n",
    "def read_ts():\n",
    "    # path\n",
    "    file = \"./cup/ds/ML-CUP23-TS.csv\"\n",
    "    test = loadtxt(file, delimiter=',', usecols=range(1, 11), dtype=np.float64)\n",
    "\n",
    "    return test\n",
    "\n",
    "# MEE for CUP\n",
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# Get the mean value of all the passed losses\n",
    "def euclidean_distance_score(y_true, y_pred):\n",
    "    return np.mean(euclidean_distance_loss(y_true, y_pred))\n",
    "\n",
    "scorer = make_scorer(euclidean_distance_score, greater_is_better=False)\n",
    "\n",
    "\n",
    "# define the NN model\n",
    "def create_model(layers=3, n_units=200, init_mode='glorot_normal',activation='relu', lmb=0.005, eta=0.004, alpha=0.08, batch_size=None): #relu\n",
    "    model = Sequential() #sequential model, linear stack of layers\n",
    "    \n",
    "    # create hidden layers with tanh activation\n",
    "    for i in range(layers - 1):  # Last layer will have ReLU activation\n",
    "        model.add(Dense(n_units, kernel_initializer=init_mode, activation=activation, kernel_regularizer=l2(lmb)))\n",
    "    \n",
    "    # create output layer with linear activation for regession task\n",
    "    model.add(Dense(3, kernel_initializer=init_mode, activation='linear'))\n",
    "\n",
    "    # use SGD optimizer\n",
    "    optimizer = SGD(learning_rate=eta, momentum=alpha)\n",
    "    \n",
    "    # Compile the model with mean_euclidean_error as the loss function and metric\n",
    "    model.compile(optimizer=optimizer, loss=euclidean_distance_loss, metrics=[euclidean_distance_loss])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# save results for the CUP\n",
    "def save_predictions_to_csv(file_path, y_pred):\n",
    "    # Generate timestamp for the date\n",
    "    timestamp = datetime.datetime.now().strftime(\"%d %b %Y\")\n",
    "\n",
    "    # Write informations to the CSV file\n",
    "    with open(file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow(['Franceschi Andrea - Marco Del Pistoia - Francesco Longobardi'])\n",
    "        writer.writerow(['FraDeLo']) \n",
    "        writer.writerow(['ML-CUP23 v1'])\n",
    "        writer.writerow([f'date ({timestamp})'])\n",
    "\n",
    "    # Write predictions to the CSV file\n",
    "    with open(file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow(['id', 'output_x', 'output_y', 'output_z'])\n",
    "        for i, row in enumerate(y_pred):\n",
    "            writer.writerow([i + 1] + list(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(x, y, epochs=300, k_fold=10):\n",
    "    param_grid = {\n",
    "        'lmb':[0.002, 0.01, 0.03],\n",
    "        'eta': [0.00125, 0.001, 0.002],\n",
    "        'alpha': [0.8, 0.9, 0.95],\n",
    "        'batch_size': [64, 110, 128]\n",
    "    }\n",
    "\n",
    "    param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "    best_params = None\n",
    "    best_loss = float('inf')  # initialize best loss with highest possible value\n",
    "\n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "\n",
    "    for params in param_combinations:\n",
    "        val_losses = []\n",
    "\n",
    "        for train_index, val_index in kf.split(x):\n",
    "            x_train, x_val = x[train_index], x[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # Create a neural network model with the specified parameters\n",
    "            model = create_model(layers=3, **params)\n",
    "\n",
    "            # Train the model on the training data (x_train, y_train) with validation data (x_val, y_val)\n",
    "            history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=params['batch_size'], verbose=0)\n",
    "\n",
    "            # Calculate the final loss on the validation set\n",
    "            val_loss = history.history['val_loss'][-1]\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        # Calculate the mean validation loss over k folds\n",
    "        mean_val_loss = np.mean(val_losses)\n",
    "\n",
    "        # Print mean validation loss for each parameter combination\n",
    "        print(f\"\\nParameters: {params}\")\n",
    "        print(f\"Mean Validation Loss: {mean_val_loss:.4f}\")\n",
    "\n",
    "        # Update the best loss and corresponding hyperparameters if necessary\n",
    "        if mean_val_loss < best_loss:\n",
    "            best_loss = mean_val_loss\n",
    "            best_params = params\n",
    "\n",
    "    # Print the best hyperparameters and corresponding scores\n",
    "    print(\"\\nBest Parameters:\", best_params)\n",
    "    print(\"Best Mean Validation Loss:\", best_loss)\n",
    "\n",
    "    return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search results:\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 64, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.5364\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 64, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 2.1638\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 64, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 2.7154\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 64, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.5557\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 64, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 2.3543\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 64, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 2.8756\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 64, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.4405\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 64, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 1.9773\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 64, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 2.6274\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 110, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.6762\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 110, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 2.7117\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 110, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 3.2431\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 110, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.7861\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 110, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 2.9735\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 110, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 3.5865\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 110, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.5756\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 110, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 2.2754\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 110, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 2.7746\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 128, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.7639\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 128, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 2.8846\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 128, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 3.4734\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 128, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.8797\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 128, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 3.1597\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 128, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 3.9246\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 128, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.6593\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 128, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 2.4068\n",
    "\n",
    "Parameters: {'alpha': 0.9, 'batch_size': 128, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 2.9151\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 64, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.7405\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 64, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 2.8811\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 64, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 3.4558\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 64, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.8506\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 64, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 3.1388\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 64, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 3.9325\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 64, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.6276\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 64, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 2.4043\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 64, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 2.9115\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 110, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 2.0665\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 110, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 3.5569\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 110, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 4.7512\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 110, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 2.2676\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 110, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 3.8705\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 110, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 5.4183\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 110, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.7949\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 110, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 3.0045\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 110, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 3.6396\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 128, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 2.2386\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 128, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 3.7801\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 128, 'eta': 0.00125, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 5.2028\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 128, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 2.4481\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 128, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 4.1036\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 128, 'eta': 0.001, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 5.9571\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 128, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Mean Validation Loss: 1.8880\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 128, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.01}\n",
    "Mean Validation Loss: 3.1685\n",
    "\n",
    "Parameters: {'alpha': 0.8, 'batch_size': 128, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.03}\n",
    "Mean Validation Loss: 3.9081\n",
    "\n",
    "Best Parameters: {'alpha': 0.9, 'batch_size': 64, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "Best Mean Validation Loss: 1.4405151963233949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x_ts, x_its, y_its):\n",
    "    # predict on internal test set\n",
    "    y_ipred = model.predict(x_its)\n",
    "    # evaluate loss with MEE\n",
    "    iloss = euclidean_distance_loss(y_its,y_ipred) \n",
    "  \n",
    "    y_pred = model.predict(x_ts)\n",
    "\n",
    "    return y_pred, K.eval(iloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_learning_curve(history, start_epoch=1, **kwargs):\n",
    "    lgd = ['Loss TR']\n",
    "    plt.plot(range(start_epoch, kwargs['epochs']), history['loss'][start_epoch:])\n",
    "    \n",
    "    if \"val_loss\" in history:\n",
    "        plt.plot(range(start_epoch, kwargs['epochs']), history['val_loss'][start_epoch:])\n",
    "        lgd.append('Loss VL')\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f'Keras Learning Curve \\n {kwargs}')\n",
    "    plt.legend(lgd)\n",
    "\n",
    "    # Check if predictions are available in the history\n",
    "    if 'predictions' in history:\n",
    "        predictions = history['predictions']\n",
    "        \n",
    "        # Plot predictions for each variable\n",
    "        for i in range(predictions.shape[1]):\n",
    "            plt.figure()\n",
    "            plt.plot(range(start_epoch, kwargs['epochs']), predictions[:, i][start_epoch:])\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(f\"Prediction Variable {i+1}\")\n",
    "            plt.title(f'Keras Learning Curve \\n {kwargs} - Prediction Variable {i+1}')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def keras_nn(ms=False, output_file='FraDeLo_ML-CUP23-TS.csv'):\n",
    "    print(\"keras start\")\n",
    "\n",
    "    file_path_tr = \"./cup/ds/ML-CUP23-TR.csv\"\n",
    "    \n",
    "    # read training set\n",
    "    x, y, x_its, y_its = read_tr(file_path_tr)\n",
    "\n",
    "    # choose model selection or hand-given parameters\n",
    "    if ms:\n",
    "        params = model_selection(x, y, epochs=300, k_fold=10)\n",
    "    else:\n",
    "        params = dict(eta=0.00125, alpha=0.9, lmb=0.002, epochs=310, batch_size=110) \n",
    "    '''\n",
    "    Best Parameters: {'alpha': 0.9, 'batch_size': 64, 'eta': 0.002, 'init_mode': 'glorot_normal', 'lmb': 0.002}\n",
    "    Best Mean Validation Loss: 1.4405151963233949\n",
    "    '''\n",
    "    #params = dict(eta=0.00125, alpha=0.9, lmb=0.002, epochs=310, batch_size=110)\n",
    "\n",
    "    # create and fit the model\n",
    "    model = create_model(eta=params['eta'], alpha=params['alpha'], lmb=params['lmb'])\n",
    "      \n",
    "    start_train_time = time.time()  # Record the starting time of the training\n",
    "    res = model.fit(x, y, validation_split=0.3, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1)\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    tr_losses = res.history['loss']\n",
    "    val_losses = res.history['val_loss']\n",
    "    \n",
    "    \n",
    "    # Predict for the three variables\n",
    "    y_pred, ts_losses = predict(model=model, x_ts=read_ts(), x_its=x_its, y_its=y_its)\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    #save_predictions_to_csv(output_file, y_pred)\n",
    "    \n",
    "    \n",
    "    print(\"TR Loss: \", tr_losses[-1])\n",
    "    print(\"VL Loss: \", val_losses[-1])\n",
    "    print(\"TS Loss: \", np.mean(ts_losses))\n",
    "    \n",
    "    training_time = end_train_time - start_train_time\n",
    "    print(\"Training Time: {:.2f} seconds\".format(training_time))\n",
    "\n",
    "    print(\"keras end\")\n",
    "    print(y_pred)\n",
    "\n",
    "    plot_learning_curve(res.history, **params)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN with Early stopping\n",
    "run the model for 1000 epochs: we got problems to plot the training curve, no problem: this function give us the # of epochs for which we got no improvements (after 10 epochs), so is commented and the correct # epochs is given to the previous function: \n",
    " results: \n",
    "6/6 [==============================] - 0s 7ms/step - loss: 1.5641 - euclidean_distance_loss: 1.0462 - val_loss: 1.8281 - val_euclidean_distance_loss: 1.3104\n",
    "Epoch 308: early stopping\n",
    "5/5 [==============================] - 0s 987us/step\n",
    "29/29 [==============================] - 0s 889us/step\n",
    "Structure of y_pred: [[  9.083963     9.398766    19.813477  ]\n",
    " [  8.494265   -37.580475    34.374172  ]\n",
    " [  9.412038     5.764892    20.003586  ]\n",
    " ...\n",
    " [  5.4391093    9.902451     0.24009445]\n",
    " [  3.7351596  -69.34859     28.1709    ]\n",
    " [-69.03668      5.9125805   11.810619  ]]\n",
    "TR Loss:  1.5640722513198853\n",
    "VL Loss:  1.8281021118164062\n",
    "TS Loss:  1.3520365953445435\n",
    "Training Time: 10.98 seconds\n",
    "keras end\n",
    "\n",
    "so run the model with 310 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def keras_nn_es(ms=False, output_file='FraDeLo_ML-CUP23-TS.csv'):\n",
    "    print(\"keras start\")\n",
    "\n",
    "    file_path_tr = \"./cup/ds/ML-CUP23-TR.csv\"\n",
    "    \n",
    "    # read training set\n",
    "    x, y, x_its, y_its = read_tr(file_path_tr)\n",
    "\n",
    "    # choose model selection or hand-given parameters\n",
    "    if ms:\n",
    "        params = model_selection(x, y)\n",
    "    else:\n",
    "        params = dict(eta=0.00125, alpha=0.9, lmb=0.002, epochs=1000, batch_size=110)\n",
    "\n",
    "    # create and fit the model\n",
    "    model = create_model(eta=params['eta'], alpha=params['alpha'], lmb=params['lmb'])\n",
    "    \n",
    "    #introduce early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    \n",
    "    start_train_time = time.time()  # Record the starting time of the training\n",
    "    res = model.fit(x, y, validation_split=0.3, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1, callbacks=[es])\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    tr_losses = res.history['loss']\n",
    "    val_losses = res.history['val_loss']\n",
    "    \n",
    "    \n",
    "    # Predict for the three variables\n",
    "    y_pred, ts_losses = predict(model=model, x_ts=read_ts(), x_its=x_its, y_its=y_its)\n",
    "    \n",
    "    # Print the structure of y_pred\n",
    "    print(\"Structure of y_pred:\", y_pred)\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    save_predictions_to_csv(output_file, y_pred)\n",
    "    \n",
    "\n",
    "    print(\"TR Loss: \", tr_losses[-1])\n",
    "    print(\"VL Loss: \", val_losses[-1])\n",
    "    print(\"TS Loss: \", np.mean(ts_losses))\n",
    "    \n",
    "    training_time = end_train_time - start_train_time\n",
    "    print(\"Training Time: {:.2f} seconds\".format(training_time))\n",
    "    print(\"keras end\")\n",
    "    plot_learning_curve(res.history, savefig=True, **params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras start\n",
      "Epoch 1/310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 28ms/step - loss: 43.2422 - euclidean_distance_loss: 42.8023 - val_loss: 43.3316 - val_euclidean_distance_loss: 42.8917\n",
      "Epoch 2/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 43.1471 - euclidean_distance_loss: 42.7073 - val_loss: 43.1897 - val_euclidean_distance_loss: 42.7500\n",
      "Epoch 3/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 42.9940 - euclidean_distance_loss: 42.5543 - val_loss: 43.0119 - val_euclidean_distance_loss: 42.5724\n",
      "Epoch 4/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 42.8142 - euclidean_distance_loss: 42.3747 - val_loss: 42.8129 - val_euclidean_distance_loss: 42.3735\n",
      "Epoch 5/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 42.6149 - euclidean_distance_loss: 42.1755 - val_loss: 42.5988 - val_euclidean_distance_loss: 42.1594\n",
      "Epoch 6/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 42.4036 - euclidean_distance_loss: 41.9643 - val_loss: 42.3684 - val_euclidean_distance_loss: 41.9290\n",
      "Epoch 7/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 42.1746 - euclidean_distance_loss: 41.7351 - val_loss: 42.1168 - val_euclidean_distance_loss: 41.6772\n",
      "Epoch 8/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 41.9237 - euclidean_distance_loss: 41.4841 - val_loss: 41.8337 - val_euclidean_distance_loss: 41.3939\n",
      "Epoch 9/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 41.6385 - euclidean_distance_loss: 41.1985 - val_loss: 41.5134 - val_euclidean_distance_loss: 41.0731\n",
      "Epoch 10/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 41.3122 - euclidean_distance_loss: 40.8718 - val_loss: 41.1429 - val_euclidean_distance_loss: 40.7022\n",
      "Epoch 11/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 40.9383 - euclidean_distance_loss: 40.4974 - val_loss: 40.7074 - val_euclidean_distance_loss: 40.2660\n",
      "Epoch 12/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 40.5033 - euclidean_distance_loss: 40.0616 - val_loss: 40.2005 - val_euclidean_distance_loss: 39.7583\n",
      "Epoch 13/310\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 39.9955 - euclidean_distance_loss: 39.5529 - val_loss: 39.6152 - val_euclidean_distance_loss: 39.1719\n",
      "Epoch 14/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 39.4127 - euclidean_distance_loss: 38.9690 - val_loss: 38.9457 - val_euclidean_distance_loss: 38.5012\n",
      "Epoch 15/310\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 38.7617 - euclidean_distance_loss: 38.3166 - val_loss: 38.1935 - val_euclidean_distance_loss: 37.7474\n",
      "Epoch 16/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 38.0363 - euclidean_distance_loss: 37.5896 - val_loss: 37.3900 - val_euclidean_distance_loss: 36.9422\n",
      "Epoch 17/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 37.2728 - euclidean_distance_loss: 36.8242 - val_loss: 36.5652 - val_euclidean_distance_loss: 36.1154\n",
      "Epoch 18/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 36.4949 - euclidean_distance_loss: 36.0443 - val_loss: 35.7587 - val_euclidean_distance_loss: 35.3067\n",
      "Epoch 19/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 35.7151 - euclidean_distance_loss: 35.2623 - val_loss: 34.9806 - val_euclidean_distance_loss: 34.5264\n",
      "Epoch 20/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 34.9299 - euclidean_distance_loss: 34.4748 - val_loss: 34.1903 - val_euclidean_distance_loss: 33.7338\n",
      "Epoch 21/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 34.0892 - euclidean_distance_loss: 33.6318 - val_loss: 33.3070 - val_euclidean_distance_loss: 32.8481\n",
      "Epoch 22/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 33.1116 - euclidean_distance_loss: 32.6517 - val_loss: 32.2604 - val_euclidean_distance_loss: 31.7989\n",
      "Epoch 23/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 31.9094 - euclidean_distance_loss: 31.4469 - val_loss: 31.0233 - val_euclidean_distance_loss: 30.5590\n",
      "Epoch 24/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 30.5159 - euclidean_distance_loss: 30.0504 - val_loss: 29.5377 - val_euclidean_distance_loss: 29.0700\n",
      "Epoch 25/310\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 28.8377 - euclidean_distance_loss: 28.3686 - val_loss: 27.7883 - val_euclidean_distance_loss: 27.3168\n",
      "Epoch 26/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 26.9137 - euclidean_distance_loss: 26.4404 - val_loss: 25.7820 - val_euclidean_distance_loss: 25.3058\n",
      "Epoch 27/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 24.7503 - euclidean_distance_loss: 24.2723 - val_loss: 23.6446 - val_euclidean_distance_loss: 23.1632\n",
      "Epoch 28/310\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 22.6085 - euclidean_distance_loss: 22.1250 - val_loss: 21.5957 - val_euclidean_distance_loss: 21.1086\n",
      "Epoch 29/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 20.5899 - euclidean_distance_loss: 20.1008 - val_loss: 19.8208 - val_euclidean_distance_loss: 19.3284\n",
      "Epoch 30/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 18.8959 - euclidean_distance_loss: 18.4017 - val_loss: 17.9312 - val_euclidean_distance_loss: 17.4342\n",
      "Epoch 31/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 16.8724 - euclidean_distance_loss: 16.3738 - val_loss: 15.6330 - val_euclidean_distance_loss: 15.1318\n",
      "Epoch 32/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 14.4599 - euclidean_distance_loss: 13.9572 - val_loss: 12.9362 - val_euclidean_distance_loss: 12.4307\n",
      "Epoch 33/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 11.7815 - euclidean_distance_loss: 11.2743 - val_loss: 10.4288 - val_euclidean_distance_loss: 9.9186\n",
      "Epoch 34/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 9.6427 - euclidean_distance_loss: 9.1305 - val_loss: 8.9753 - val_euclidean_distance_loss: 8.4599\n",
      "Epoch 35/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 8.4381 - euclidean_distance_loss: 7.9211 - val_loss: 8.3139 - val_euclidean_distance_loss: 7.7945\n",
      "Epoch 36/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.7701 - euclidean_distance_loss: 7.2499 - val_loss: 7.8573 - val_euclidean_distance_loss: 7.3360\n",
      "Epoch 37/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 7.2525 - euclidean_distance_loss: 6.7311 - val_loss: 7.1263 - val_euclidean_distance_loss: 6.6049\n",
      "Epoch 38/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.5785 - euclidean_distance_loss: 6.0572 - val_loss: 6.4663 - val_euclidean_distance_loss: 5.9453\n",
      "Epoch 39/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 6.0997 - euclidean_distance_loss: 5.5786 - val_loss: 6.1709 - val_euclidean_distance_loss: 5.6496\n",
      "Epoch 40/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.7734 - euclidean_distance_loss: 5.2516 - val_loss: 5.7568 - val_euclidean_distance_loss: 5.2342\n",
      "Epoch 41/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.4766 - euclidean_distance_loss: 4.9535 - val_loss: 5.5079 - val_euclidean_distance_loss: 4.9838\n",
      "Epoch 42/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.2577 - euclidean_distance_loss: 4.7332 - val_loss: 5.3085 - val_euclidean_distance_loss: 4.7833\n",
      "Epoch 43/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 5.0909 - euclidean_distance_loss: 4.5655 - val_loss: 5.1710 - val_euclidean_distance_loss: 4.6454\n",
      "Epoch 44/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.9584 - euclidean_distance_loss: 4.4326 - val_loss: 5.0353 - val_euclidean_distance_loss: 4.5093\n",
      "Epoch 45/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.8342 - euclidean_distance_loss: 4.3081 - val_loss: 4.9193 - val_euclidean_distance_loss: 4.3930\n",
      "Epoch 46/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.7245 - euclidean_distance_loss: 4.1980 - val_loss: 4.8166 - val_euclidean_distance_loss: 4.2899\n",
      "Epoch 47/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.6101 - euclidean_distance_loss: 4.0833 - val_loss: 4.7136 - val_euclidean_distance_loss: 4.1866\n",
      "Epoch 48/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.5156 - euclidean_distance_loss: 3.9885 - val_loss: 4.6260 - val_euclidean_distance_loss: 4.0989\n",
      "Epoch 49/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.4357 - euclidean_distance_loss: 3.9086 - val_loss: 4.5422 - val_euclidean_distance_loss: 4.0148\n",
      "Epoch 50/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.3624 - euclidean_distance_loss: 3.8349 - val_loss: 4.4673 - val_euclidean_distance_loss: 3.9397\n",
      "Epoch 51/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.2797 - euclidean_distance_loss: 3.7520 - val_loss: 4.4121 - val_euclidean_distance_loss: 3.8845\n",
      "Epoch 52/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.2257 - euclidean_distance_loss: 3.6979 - val_loss: 4.3333 - val_euclidean_distance_loss: 3.8053\n",
      "Epoch 53/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.1377 - euclidean_distance_loss: 3.6097 - val_loss: 4.2711 - val_euclidean_distance_loss: 3.7431\n",
      "Epoch 54/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.0830 - euclidean_distance_loss: 3.5548 - val_loss: 4.1865 - val_euclidean_distance_loss: 3.6581\n",
      "Epoch 55/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.0025 - euclidean_distance_loss: 3.4740 - val_loss: 4.1390 - val_euclidean_distance_loss: 3.6105\n",
      "Epoch 56/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.9518 - euclidean_distance_loss: 3.4232 - val_loss: 4.0904 - val_euclidean_distance_loss: 3.5615\n",
      "Epoch 57/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.9084 - euclidean_distance_loss: 3.3794 - val_loss: 4.0381 - val_euclidean_distance_loss: 3.5090\n",
      "Epoch 58/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.8445 - euclidean_distance_loss: 3.3155 - val_loss: 4.0004 - val_euclidean_distance_loss: 3.4714\n",
      "Epoch 59/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.8044 - euclidean_distance_loss: 3.2753 - val_loss: 3.9684 - val_euclidean_distance_loss: 3.4391\n",
      "Epoch 60/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.7497 - euclidean_distance_loss: 3.2202 - val_loss: 3.8751 - val_euclidean_distance_loss: 3.3454\n",
      "Epoch 61/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.6920 - euclidean_distance_loss: 3.1621 - val_loss: 3.8163 - val_euclidean_distance_loss: 3.2863\n",
      "Epoch 62/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.6459 - euclidean_distance_loss: 3.1160 - val_loss: 3.7832 - val_euclidean_distance_loss: 3.2533\n",
      "Epoch 63/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.5962 - euclidean_distance_loss: 3.0662 - val_loss: 3.7330 - val_euclidean_distance_loss: 3.2028\n",
      "Epoch 64/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.5517 - euclidean_distance_loss: 3.0214 - val_loss: 3.6883 - val_euclidean_distance_loss: 3.1580\n",
      "Epoch 65/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.5128 - euclidean_distance_loss: 2.9825 - val_loss: 3.6686 - val_euclidean_distance_loss: 3.1382\n",
      "Epoch 66/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.4743 - euclidean_distance_loss: 2.9439 - val_loss: 3.6291 - val_euclidean_distance_loss: 3.0985\n",
      "Epoch 67/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.4283 - euclidean_distance_loss: 2.8976 - val_loss: 3.5943 - val_euclidean_distance_loss: 3.0635\n",
      "Epoch 68/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.3882 - euclidean_distance_loss: 2.8573 - val_loss: 3.5561 - val_euclidean_distance_loss: 3.0250\n",
      "Epoch 69/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.3592 - euclidean_distance_loss: 2.8281 - val_loss: 3.4989 - val_euclidean_distance_loss: 2.9677\n",
      "Epoch 70/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.3185 - euclidean_distance_loss: 2.7872 - val_loss: 3.4948 - val_euclidean_distance_loss: 2.9634\n",
      "Epoch 71/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.2839 - euclidean_distance_loss: 2.7523 - val_loss: 3.4551 - val_euclidean_distance_loss: 2.9235\n",
      "Epoch 72/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.2525 - euclidean_distance_loss: 2.7209 - val_loss: 3.4326 - val_euclidean_distance_loss: 2.9010\n",
      "Epoch 73/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.2140 - euclidean_distance_loss: 2.6824 - val_loss: 3.4098 - val_euclidean_distance_loss: 2.8779\n",
      "Epoch 74/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.2091 - euclidean_distance_loss: 2.6770 - val_loss: 3.3553 - val_euclidean_distance_loss: 2.8231\n",
      "Epoch 75/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.1479 - euclidean_distance_loss: 2.6158 - val_loss: 3.3440 - val_euclidean_distance_loss: 2.8121\n",
      "Epoch 76/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.1390 - euclidean_distance_loss: 2.6069 - val_loss: 3.3128 - val_euclidean_distance_loss: 2.7803\n",
      "Epoch 77/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.0947 - euclidean_distance_loss: 2.5621 - val_loss: 3.2668 - val_euclidean_distance_loss: 2.7342\n",
      "Epoch 78/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.0642 - euclidean_distance_loss: 2.5317 - val_loss: 3.2513 - val_euclidean_distance_loss: 2.7187\n",
      "Epoch 79/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.0251 - euclidean_distance_loss: 2.4925 - val_loss: 3.2236 - val_euclidean_distance_loss: 2.6908\n",
      "Epoch 80/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.0004 - euclidean_distance_loss: 2.4676 - val_loss: 3.1935 - val_euclidean_distance_loss: 2.6607\n",
      "Epoch 81/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.0052 - euclidean_distance_loss: 2.4724 - val_loss: 3.1745 - val_euclidean_distance_loss: 2.6415\n",
      "Epoch 82/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.9530 - euclidean_distance_loss: 2.4198 - val_loss: 3.1344 - val_euclidean_distance_loss: 2.6011\n",
      "Epoch 83/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.9272 - euclidean_distance_loss: 2.3939 - val_loss: 3.1625 - val_euclidean_distance_loss: 2.6292\n",
      "Epoch 84/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.9090 - euclidean_distance_loss: 2.3759 - val_loss: 3.1295 - val_euclidean_distance_loss: 2.5962\n",
      "Epoch 85/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.8953 - euclidean_distance_loss: 2.3618 - val_loss: 3.0785 - val_euclidean_distance_loss: 2.5449\n",
      "Epoch 86/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.8624 - euclidean_distance_loss: 2.3288 - val_loss: 3.0652 - val_euclidean_distance_loss: 2.5317\n",
      "Epoch 87/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.8576 - euclidean_distance_loss: 2.3241 - val_loss: 3.0659 - val_euclidean_distance_loss: 2.5324\n",
      "Epoch 88/310\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8246 - euclidean_distance_loss: 2.2908 - val_loss: 3.0132 - val_euclidean_distance_loss: 2.4790\n",
      "Epoch 89/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.7719 - euclidean_distance_loss: 2.2378 - val_loss: 3.0423 - val_euclidean_distance_loss: 2.5084\n",
      "Epoch 90/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.7840 - euclidean_distance_loss: 2.2501 - val_loss: 3.0066 - val_euclidean_distance_loss: 2.4723\n",
      "Epoch 91/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.7605 - euclidean_distance_loss: 2.2262 - val_loss: 2.9655 - val_euclidean_distance_loss: 2.4312\n",
      "Epoch 92/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.7243 - euclidean_distance_loss: 2.1899 - val_loss: 2.9784 - val_euclidean_distance_loss: 2.4440\n",
      "Epoch 93/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.7245 - euclidean_distance_loss: 2.1902 - val_loss: 2.9102 - val_euclidean_distance_loss: 2.3758\n",
      "Epoch 94/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6840 - euclidean_distance_loss: 2.1495 - val_loss: 2.9036 - val_euclidean_distance_loss: 2.3689\n",
      "Epoch 95/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6536 - euclidean_distance_loss: 2.1188 - val_loss: 2.8818 - val_euclidean_distance_loss: 2.3470\n",
      "Epoch 96/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6372 - euclidean_distance_loss: 2.1026 - val_loss: 2.8521 - val_euclidean_distance_loss: 2.3174\n",
      "Epoch 97/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6164 - euclidean_distance_loss: 2.0816 - val_loss: 2.8543 - val_euclidean_distance_loss: 2.3193\n",
      "Epoch 98/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6120 - euclidean_distance_loss: 2.0771 - val_loss: 2.8456 - val_euclidean_distance_loss: 2.3108\n",
      "Epoch 99/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.5842 - euclidean_distance_loss: 2.0493 - val_loss: 2.8123 - val_euclidean_distance_loss: 2.2772\n",
      "Epoch 100/310\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.5645 - euclidean_distance_loss: 2.0294 - val_loss: 2.8043 - val_euclidean_distance_loss: 2.2692\n",
      "Epoch 101/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.5380 - euclidean_distance_loss: 2.0029 - val_loss: 2.8098 - val_euclidean_distance_loss: 2.2747\n",
      "Epoch 102/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.5417 - euclidean_distance_loss: 2.0065 - val_loss: 2.7588 - val_euclidean_distance_loss: 2.2235\n",
      "Epoch 103/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.5278 - euclidean_distance_loss: 1.9922 - val_loss: 2.7409 - val_euclidean_distance_loss: 2.2054\n",
      "Epoch 104/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.5099 - euclidean_distance_loss: 1.9746 - val_loss: 2.7445 - val_euclidean_distance_loss: 2.2091\n",
      "Epoch 105/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.4808 - euclidean_distance_loss: 1.9453 - val_loss: 2.7153 - val_euclidean_distance_loss: 2.1796\n",
      "Epoch 106/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.4645 - euclidean_distance_loss: 1.9288 - val_loss: 2.7387 - val_euclidean_distance_loss: 2.2032\n",
      "Epoch 107/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.4627 - euclidean_distance_loss: 1.9273 - val_loss: 2.6860 - val_euclidean_distance_loss: 2.1504\n",
      "Epoch 108/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.4367 - euclidean_distance_loss: 1.9010 - val_loss: 2.6809 - val_euclidean_distance_loss: 2.1450\n",
      "Epoch 109/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.4298 - euclidean_distance_loss: 1.8941 - val_loss: 2.6930 - val_euclidean_distance_loss: 2.1574\n",
      "Epoch 110/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.4559 - euclidean_distance_loss: 1.9201 - val_loss: 2.6451 - val_euclidean_distance_loss: 2.1093\n",
      "Epoch 111/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.4276 - euclidean_distance_loss: 1.8919 - val_loss: 2.6431 - val_euclidean_distance_loss: 2.1075\n",
      "Epoch 112/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.4003 - euclidean_distance_loss: 1.8645 - val_loss: 2.6246 - val_euclidean_distance_loss: 2.0888\n",
      "Epoch 113/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.3761 - euclidean_distance_loss: 1.8402 - val_loss: 2.6387 - val_euclidean_distance_loss: 2.1029\n",
      "Epoch 114/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.3599 - euclidean_distance_loss: 1.8241 - val_loss: 2.6017 - val_euclidean_distance_loss: 2.0657\n",
      "Epoch 115/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.3590 - euclidean_distance_loss: 1.8230 - val_loss: 2.5816 - val_euclidean_distance_loss: 2.0456\n",
      "Epoch 116/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.3418 - euclidean_distance_loss: 1.8058 - val_loss: 2.6017 - val_euclidean_distance_loss: 2.0656\n",
      "Epoch 117/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.3350 - euclidean_distance_loss: 1.7989 - val_loss: 2.5663 - val_euclidean_distance_loss: 2.0302\n",
      "Epoch 118/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.3099 - euclidean_distance_loss: 1.7739 - val_loss: 2.5731 - val_euclidean_distance_loss: 2.0370\n",
      "Epoch 119/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.3251 - euclidean_distance_loss: 1.7889 - val_loss: 2.5677 - val_euclidean_distance_loss: 2.0314\n",
      "Epoch 120/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2979 - euclidean_distance_loss: 1.7618 - val_loss: 2.5668 - val_euclidean_distance_loss: 2.0309\n",
      "Epoch 121/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2897 - euclidean_distance_loss: 1.7536 - val_loss: 2.5220 - val_euclidean_distance_loss: 1.9858\n",
      "Epoch 122/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.2762 - euclidean_distance_loss: 1.7399 - val_loss: 2.5367 - val_euclidean_distance_loss: 2.0006\n",
      "Epoch 123/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.2722 - euclidean_distance_loss: 1.7361 - val_loss: 2.5122 - val_euclidean_distance_loss: 1.9759\n",
      "Epoch 124/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.2701 - euclidean_distance_loss: 1.7339 - val_loss: 2.5047 - val_euclidean_distance_loss: 1.9684\n",
      "Epoch 125/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.2721 - euclidean_distance_loss: 1.7357 - val_loss: 2.4957 - val_euclidean_distance_loss: 1.9594\n",
      "Epoch 126/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2472 - euclidean_distance_loss: 1.7109 - val_loss: 2.5121 - val_euclidean_distance_loss: 1.9757\n",
      "Epoch 127/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2298 - euclidean_distance_loss: 1.6934 - val_loss: 2.4651 - val_euclidean_distance_loss: 1.9287\n",
      "Epoch 128/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.2105 - euclidean_distance_loss: 1.6741 - val_loss: 2.4786 - val_euclidean_distance_loss: 1.9422\n",
      "Epoch 129/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2153 - euclidean_distance_loss: 1.6790 - val_loss: 2.4497 - val_euclidean_distance_loss: 1.9134\n",
      "Epoch 130/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2245 - euclidean_distance_loss: 1.6881 - val_loss: 2.4464 - val_euclidean_distance_loss: 1.9101\n",
      "Epoch 131/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2330 - euclidean_distance_loss: 1.6967 - val_loss: 2.4394 - val_euclidean_distance_loss: 1.9030\n",
      "Epoch 132/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2304 - euclidean_distance_loss: 1.6941 - val_loss: 2.5105 - val_euclidean_distance_loss: 1.9742\n",
      "Epoch 133/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2292 - euclidean_distance_loss: 1.6929 - val_loss: 2.5295 - val_euclidean_distance_loss: 1.9931\n",
      "Epoch 134/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.2058 - euclidean_distance_loss: 1.6693 - val_loss: 2.4157 - val_euclidean_distance_loss: 1.8793\n",
      "Epoch 135/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.1826 - euclidean_distance_loss: 1.6463 - val_loss: 2.4105 - val_euclidean_distance_loss: 1.8741\n",
      "Epoch 136/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.1770 - euclidean_distance_loss: 1.6407 - val_loss: 2.4053 - val_euclidean_distance_loss: 1.8689\n",
      "Epoch 137/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.1760 - euclidean_distance_loss: 1.6396 - val_loss: 2.4084 - val_euclidean_distance_loss: 1.8720\n",
      "Epoch 138/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.1487 - euclidean_distance_loss: 1.6124 - val_loss: 2.4555 - val_euclidean_distance_loss: 1.9193\n",
      "Epoch 139/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.1705 - euclidean_distance_loss: 1.6342 - val_loss: 2.4143 - val_euclidean_distance_loss: 1.8780\n",
      "Epoch 140/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.1524 - euclidean_distance_loss: 1.6161 - val_loss: 2.3594 - val_euclidean_distance_loss: 1.8232\n",
      "Epoch 141/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.1432 - euclidean_distance_loss: 1.6071 - val_loss: 2.3728 - val_euclidean_distance_loss: 1.8366\n",
      "Epoch 142/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.1254 - euclidean_distance_loss: 1.5890 - val_loss: 2.3393 - val_euclidean_distance_loss: 1.8030\n",
      "Epoch 143/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.1137 - euclidean_distance_loss: 1.5774 - val_loss: 2.3228 - val_euclidean_distance_loss: 1.7866\n",
      "Epoch 144/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0939 - euclidean_distance_loss: 1.5577 - val_loss: 2.3386 - val_euclidean_distance_loss: 1.8025\n",
      "Epoch 145/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0886 - euclidean_distance_loss: 1.5524 - val_loss: 2.3221 - val_euclidean_distance_loss: 1.7858\n",
      "Epoch 146/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0875 - euclidean_distance_loss: 1.5513 - val_loss: 2.3133 - val_euclidean_distance_loss: 1.7771\n",
      "Epoch 147/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0911 - euclidean_distance_loss: 1.5550 - val_loss: 2.2945 - val_euclidean_distance_loss: 1.7584\n",
      "Epoch 148/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0777 - euclidean_distance_loss: 1.5415 - val_loss: 2.2971 - val_euclidean_distance_loss: 1.7609\n",
      "Epoch 149/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0529 - euclidean_distance_loss: 1.5168 - val_loss: 2.3101 - val_euclidean_distance_loss: 1.7741\n",
      "Epoch 150/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0412 - euclidean_distance_loss: 1.5052 - val_loss: 2.3358 - val_euclidean_distance_loss: 1.7998\n",
      "Epoch 151/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0390 - euclidean_distance_loss: 1.5029 - val_loss: 2.2964 - val_euclidean_distance_loss: 1.7603\n",
      "Epoch 152/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0210 - euclidean_distance_loss: 1.4851 - val_loss: 2.2717 - val_euclidean_distance_loss: 1.7357\n",
      "Epoch 153/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0263 - euclidean_distance_loss: 1.4903 - val_loss: 2.2992 - val_euclidean_distance_loss: 1.7632\n",
      "Epoch 154/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0231 - euclidean_distance_loss: 1.4871 - val_loss: 2.2641 - val_euclidean_distance_loss: 1.7281\n",
      "Epoch 155/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.0249 - euclidean_distance_loss: 1.4890 - val_loss: 2.2844 - val_euclidean_distance_loss: 1.7485\n",
      "Epoch 156/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0433 - euclidean_distance_loss: 1.5073 - val_loss: 2.2415 - val_euclidean_distance_loss: 1.7056\n",
      "Epoch 157/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0285 - euclidean_distance_loss: 1.4925 - val_loss: 2.2662 - val_euclidean_distance_loss: 1.7303\n",
      "Epoch 158/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0144 - euclidean_distance_loss: 1.4785 - val_loss: 2.2552 - val_euclidean_distance_loss: 1.7194\n",
      "Epoch 159/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0160 - euclidean_distance_loss: 1.4802 - val_loss: 2.2741 - val_euclidean_distance_loss: 1.7380\n",
      "Epoch 160/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0216 - euclidean_distance_loss: 1.4856 - val_loss: 2.2622 - val_euclidean_distance_loss: 1.7264\n",
      "Epoch 161/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0108 - euclidean_distance_loss: 1.4749 - val_loss: 2.3206 - val_euclidean_distance_loss: 1.7846\n",
      "Epoch 162/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.0561 - euclidean_distance_loss: 1.5202 - val_loss: 2.2996 - val_euclidean_distance_loss: 1.7639\n",
      "Epoch 163/310\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.0314 - euclidean_distance_loss: 1.4959 - val_loss: 2.2531 - val_euclidean_distance_loss: 1.7174\n",
      "Epoch 164/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0247 - euclidean_distance_loss: 1.4890 - val_loss: 2.1922 - val_euclidean_distance_loss: 1.6566\n",
      "Epoch 165/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0060 - euclidean_distance_loss: 1.4704 - val_loss: 2.2464 - val_euclidean_distance_loss: 1.7108\n",
      "Epoch 166/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.0027 - euclidean_distance_loss: 1.4671 - val_loss: 2.2245 - val_euclidean_distance_loss: 1.6890\n",
      "Epoch 167/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.0056 - euclidean_distance_loss: 1.4700 - val_loss: 2.2154 - val_euclidean_distance_loss: 1.6797\n",
      "Epoch 168/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9526 - euclidean_distance_loss: 1.4170 - val_loss: 2.1954 - val_euclidean_distance_loss: 1.6598\n",
      "Epoch 169/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9491 - euclidean_distance_loss: 1.4135 - val_loss: 2.1952 - val_euclidean_distance_loss: 1.6597\n",
      "Epoch 170/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9462 - euclidean_distance_loss: 1.4108 - val_loss: 2.1719 - val_euclidean_distance_loss: 1.6363\n",
      "Epoch 171/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.9484 - euclidean_distance_loss: 1.4128 - val_loss: 2.1866 - val_euclidean_distance_loss: 1.6513\n",
      "Epoch 172/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9603 - euclidean_distance_loss: 1.4252 - val_loss: 2.1821 - val_euclidean_distance_loss: 1.6467\n",
      "Epoch 173/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9461 - euclidean_distance_loss: 1.4106 - val_loss: 2.2062 - val_euclidean_distance_loss: 1.6710\n",
      "Epoch 174/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9455 - euclidean_distance_loss: 1.4104 - val_loss: 2.1927 - val_euclidean_distance_loss: 1.6574\n",
      "Epoch 175/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9597 - euclidean_distance_loss: 1.4244 - val_loss: 2.1846 - val_euclidean_distance_loss: 1.6496\n",
      "Epoch 176/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9628 - euclidean_distance_loss: 1.4278 - val_loss: 2.1920 - val_euclidean_distance_loss: 1.6567\n",
      "Epoch 177/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9768 - euclidean_distance_loss: 1.4416 - val_loss: 2.1468 - val_euclidean_distance_loss: 1.6117\n",
      "Epoch 178/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9555 - euclidean_distance_loss: 1.4204 - val_loss: 2.1475 - val_euclidean_distance_loss: 1.6124\n",
      "Epoch 179/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9638 - euclidean_distance_loss: 1.4287 - val_loss: 2.1746 - val_euclidean_distance_loss: 1.6395\n",
      "Epoch 180/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.9366 - euclidean_distance_loss: 1.4016 - val_loss: 2.1816 - val_euclidean_distance_loss: 1.6466\n",
      "Epoch 181/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9261 - euclidean_distance_loss: 1.3913 - val_loss: 2.1859 - val_euclidean_distance_loss: 1.6510\n",
      "Epoch 182/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9173 - euclidean_distance_loss: 1.3824 - val_loss: 2.1590 - val_euclidean_distance_loss: 1.6242\n",
      "Epoch 183/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9111 - euclidean_distance_loss: 1.3763 - val_loss: 2.1445 - val_euclidean_distance_loss: 1.6097\n",
      "Epoch 184/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8827 - euclidean_distance_loss: 1.3479 - val_loss: 2.1497 - val_euclidean_distance_loss: 1.6150\n",
      "Epoch 185/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8619 - euclidean_distance_loss: 1.3271 - val_loss: 2.0922 - val_euclidean_distance_loss: 1.5576\n",
      "Epoch 186/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8649 - euclidean_distance_loss: 1.3304 - val_loss: 2.1376 - val_euclidean_distance_loss: 1.6030\n",
      "Epoch 187/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8801 - euclidean_distance_loss: 1.3455 - val_loss: 2.0971 - val_euclidean_distance_loss: 1.5626\n",
      "Epoch 188/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8617 - euclidean_distance_loss: 1.3272 - val_loss: 2.1145 - val_euclidean_distance_loss: 1.5802\n",
      "Epoch 189/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8408 - euclidean_distance_loss: 1.3065 - val_loss: 2.0629 - val_euclidean_distance_loss: 1.5286\n",
      "Epoch 190/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8320 - euclidean_distance_loss: 1.2977 - val_loss: 2.1087 - val_euclidean_distance_loss: 1.5744\n",
      "Epoch 191/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8405 - euclidean_distance_loss: 1.3062 - val_loss: 2.1266 - val_euclidean_distance_loss: 1.5924\n",
      "Epoch 192/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8318 - euclidean_distance_loss: 1.2975 - val_loss: 2.0816 - val_euclidean_distance_loss: 1.5472\n",
      "Epoch 193/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.8260 - euclidean_distance_loss: 1.2918 - val_loss: 2.0702 - val_euclidean_distance_loss: 1.5361\n",
      "Epoch 194/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8425 - euclidean_distance_loss: 1.3083 - val_loss: 2.0990 - val_euclidean_distance_loss: 1.5649\n",
      "Epoch 195/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8418 - euclidean_distance_loss: 1.3078 - val_loss: 2.0509 - val_euclidean_distance_loss: 1.5169\n",
      "Epoch 196/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8328 - euclidean_distance_loss: 1.2988 - val_loss: 2.0917 - val_euclidean_distance_loss: 1.5576\n",
      "Epoch 197/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8336 - euclidean_distance_loss: 1.2995 - val_loss: 2.0718 - val_euclidean_distance_loss: 1.5378\n",
      "Epoch 198/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8275 - euclidean_distance_loss: 1.2935 - val_loss: 2.0745 - val_euclidean_distance_loss: 1.5405\n",
      "Epoch 199/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8000 - euclidean_distance_loss: 1.2661 - val_loss: 2.0630 - val_euclidean_distance_loss: 1.5293\n",
      "Epoch 200/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7975 - euclidean_distance_loss: 1.2637 - val_loss: 2.0313 - val_euclidean_distance_loss: 1.4974\n",
      "Epoch 201/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7872 - euclidean_distance_loss: 1.2534 - val_loss: 2.0672 - val_euclidean_distance_loss: 1.5335\n",
      "Epoch 202/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8041 - euclidean_distance_loss: 1.2705 - val_loss: 2.0387 - val_euclidean_distance_loss: 1.5051\n",
      "Epoch 203/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8030 - euclidean_distance_loss: 1.2694 - val_loss: 2.1022 - val_euclidean_distance_loss: 1.5687\n",
      "Epoch 204/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8010 - euclidean_distance_loss: 1.2675 - val_loss: 2.0572 - val_euclidean_distance_loss: 1.5238\n",
      "Epoch 205/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8060 - euclidean_distance_loss: 1.2725 - val_loss: 2.0721 - val_euclidean_distance_loss: 1.5387\n",
      "Epoch 206/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7962 - euclidean_distance_loss: 1.2628 - val_loss: 2.0399 - val_euclidean_distance_loss: 1.5064\n",
      "Epoch 207/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7863 - euclidean_distance_loss: 1.2529 - val_loss: 2.0950 - val_euclidean_distance_loss: 1.5617\n",
      "Epoch 208/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8070 - euclidean_distance_loss: 1.2737 - val_loss: 2.1104 - val_euclidean_distance_loss: 1.5769\n",
      "Epoch 209/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.8576 - euclidean_distance_loss: 1.3242 - val_loss: 2.0769 - val_euclidean_distance_loss: 1.5438\n",
      "Epoch 210/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7844 - euclidean_distance_loss: 1.2512 - val_loss: 2.0578 - val_euclidean_distance_loss: 1.5244\n",
      "Epoch 211/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7662 - euclidean_distance_loss: 1.2330 - val_loss: 2.0317 - val_euclidean_distance_loss: 1.4987\n",
      "Epoch 212/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7534 - euclidean_distance_loss: 1.2204 - val_loss: 2.0095 - val_euclidean_distance_loss: 1.4765\n",
      "Epoch 213/310\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7376 - euclidean_distance_loss: 1.2046 - val_loss: 2.0170 - val_euclidean_distance_loss: 1.4841\n",
      "Epoch 214/310\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.7369 - euclidean_distance_loss: 1.2039 - val_loss: 2.0324 - val_euclidean_distance_loss: 1.4996\n",
      "Epoch 215/310\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.7517 - euclidean_distance_loss: 1.2188 - val_loss: 2.0085 - val_euclidean_distance_loss: 1.4754\n",
      "Epoch 216/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7558 - euclidean_distance_loss: 1.2229 - val_loss: 2.0147 - val_euclidean_distance_loss: 1.4820\n",
      "Epoch 217/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7347 - euclidean_distance_loss: 1.2020 - val_loss: 1.9896 - val_euclidean_distance_loss: 1.4569\n",
      "Epoch 218/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7271 - euclidean_distance_loss: 1.1945 - val_loss: 1.9739 - val_euclidean_distance_loss: 1.4413\n",
      "Epoch 219/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7190 - euclidean_distance_loss: 1.1864 - val_loss: 1.9786 - val_euclidean_distance_loss: 1.4460\n",
      "Epoch 220/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7086 - euclidean_distance_loss: 1.1761 - val_loss: 1.9832 - val_euclidean_distance_loss: 1.4506\n",
      "Epoch 221/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7251 - euclidean_distance_loss: 1.1927 - val_loss: 1.9860 - val_euclidean_distance_loss: 1.4538\n",
      "Epoch 222/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7057 - euclidean_distance_loss: 1.1734 - val_loss: 1.9768 - val_euclidean_distance_loss: 1.4444\n",
      "Epoch 223/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7057 - euclidean_distance_loss: 1.1734 - val_loss: 1.9731 - val_euclidean_distance_loss: 1.4409\n",
      "Epoch 224/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.6934 - euclidean_distance_loss: 1.1612 - val_loss: 1.9586 - val_euclidean_distance_loss: 1.4264\n",
      "Epoch 225/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7013 - euclidean_distance_loss: 1.1691 - val_loss: 1.9764 - val_euclidean_distance_loss: 1.4443\n",
      "Epoch 226/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7373 - euclidean_distance_loss: 1.2053 - val_loss: 2.0162 - val_euclidean_distance_loss: 1.4843\n",
      "Epoch 227/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7401 - euclidean_distance_loss: 1.2081 - val_loss: 2.0510 - val_euclidean_distance_loss: 1.5191\n",
      "Epoch 228/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7513 - euclidean_distance_loss: 1.2193 - val_loss: 2.0323 - val_euclidean_distance_loss: 1.5006\n",
      "Epoch 229/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7403 - euclidean_distance_loss: 1.2087 - val_loss: 1.9515 - val_euclidean_distance_loss: 1.4198\n",
      "Epoch 230/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7093 - euclidean_distance_loss: 1.1775 - val_loss: 1.9600 - val_euclidean_distance_loss: 1.4283\n",
      "Epoch 231/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7017 - euclidean_distance_loss: 1.1701 - val_loss: 1.9307 - val_euclidean_distance_loss: 1.3992\n",
      "Epoch 232/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7152 - euclidean_distance_loss: 1.1836 - val_loss: 1.9336 - val_euclidean_distance_loss: 1.4019\n",
      "Epoch 233/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7040 - euclidean_distance_loss: 1.1725 - val_loss: 2.0079 - val_euclidean_distance_loss: 1.4764\n",
      "Epoch 234/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7539 - euclidean_distance_loss: 1.2225 - val_loss: 1.9335 - val_euclidean_distance_loss: 1.4021\n",
      "Epoch 235/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7240 - euclidean_distance_loss: 1.1927 - val_loss: 1.9846 - val_euclidean_distance_loss: 1.4535\n",
      "Epoch 236/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7093 - euclidean_distance_loss: 1.1780 - val_loss: 1.9802 - val_euclidean_distance_loss: 1.4490\n",
      "Epoch 237/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7181 - euclidean_distance_loss: 1.1871 - val_loss: 1.9195 - val_euclidean_distance_loss: 1.3885\n",
      "Epoch 238/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6966 - euclidean_distance_loss: 1.1655 - val_loss: 1.9364 - val_euclidean_distance_loss: 1.4054\n",
      "Epoch 239/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6584 - euclidean_distance_loss: 1.1274 - val_loss: 1.9295 - val_euclidean_distance_loss: 1.3986\n",
      "Epoch 240/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.6510 - euclidean_distance_loss: 1.1202 - val_loss: 1.9028 - val_euclidean_distance_loss: 1.3719\n",
      "Epoch 241/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6437 - euclidean_distance_loss: 1.1128 - val_loss: 1.9755 - val_euclidean_distance_loss: 1.4448\n",
      "Epoch 242/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6674 - euclidean_distance_loss: 1.1367 - val_loss: 1.9718 - val_euclidean_distance_loss: 1.4410\n",
      "Epoch 243/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6785 - euclidean_distance_loss: 1.1479 - val_loss: 1.9178 - val_euclidean_distance_loss: 1.3873\n",
      "Epoch 244/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.6621 - euclidean_distance_loss: 1.1316 - val_loss: 1.9562 - val_euclidean_distance_loss: 1.4255\n",
      "Epoch 245/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6864 - euclidean_distance_loss: 1.1559 - val_loss: 1.9335 - val_euclidean_distance_loss: 1.4031\n",
      "Epoch 246/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6551 - euclidean_distance_loss: 1.1247 - val_loss: 1.9276 - val_euclidean_distance_loss: 1.3973\n",
      "Epoch 247/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6409 - euclidean_distance_loss: 1.1105 - val_loss: 1.9030 - val_euclidean_distance_loss: 1.3728\n",
      "Epoch 248/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.6310 - euclidean_distance_loss: 1.1008 - val_loss: 1.8951 - val_euclidean_distance_loss: 1.3649\n",
      "Epoch 249/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6578 - euclidean_distance_loss: 1.1277 - val_loss: 1.9012 - val_euclidean_distance_loss: 1.3713\n",
      "Epoch 250/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6370 - euclidean_distance_loss: 1.1071 - val_loss: 1.8769 - val_euclidean_distance_loss: 1.3468\n",
      "Epoch 251/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6275 - euclidean_distance_loss: 1.0976 - val_loss: 1.9042 - val_euclidean_distance_loss: 1.3742\n",
      "Epoch 252/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6414 - euclidean_distance_loss: 1.1115 - val_loss: 1.9256 - val_euclidean_distance_loss: 1.3957\n",
      "Epoch 253/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6517 - euclidean_distance_loss: 1.1217 - val_loss: 1.8884 - val_euclidean_distance_loss: 1.3587\n",
      "Epoch 254/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6342 - euclidean_distance_loss: 1.1046 - val_loss: 1.8711 - val_euclidean_distance_loss: 1.3413\n",
      "Epoch 255/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6140 - euclidean_distance_loss: 1.0843 - val_loss: 1.8865 - val_euclidean_distance_loss: 1.3570\n",
      "Epoch 256/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6130 - euclidean_distance_loss: 1.0836 - val_loss: 1.8829 - val_euclidean_distance_loss: 1.3536\n",
      "Epoch 257/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.6260 - euclidean_distance_loss: 1.0966 - val_loss: 1.9170 - val_euclidean_distance_loss: 1.3877\n",
      "Epoch 258/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.6339 - euclidean_distance_loss: 1.1047 - val_loss: 1.9548 - val_euclidean_distance_loss: 1.4257\n",
      "Epoch 259/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6701 - euclidean_distance_loss: 1.1408 - val_loss: 1.9255 - val_euclidean_distance_loss: 1.3964\n",
      "Epoch 260/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6834 - euclidean_distance_loss: 1.1544 - val_loss: 1.9523 - val_euclidean_distance_loss: 1.4232\n",
      "Epoch 261/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6696 - euclidean_distance_loss: 1.1405 - val_loss: 1.8753 - val_euclidean_distance_loss: 1.3464\n",
      "Epoch 262/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.6212 - euclidean_distance_loss: 1.0923 - val_loss: 1.9090 - val_euclidean_distance_loss: 1.3801\n",
      "Epoch 263/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.6153 - euclidean_distance_loss: 1.0865 - val_loss: 1.8615 - val_euclidean_distance_loss: 1.3327\n",
      "Epoch 264/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.6317 - euclidean_distance_loss: 1.1029 - val_loss: 1.8703 - val_euclidean_distance_loss: 1.3418\n",
      "Epoch 265/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6039 - euclidean_distance_loss: 1.0754 - val_loss: 1.8439 - val_euclidean_distance_loss: 1.3154\n",
      "Epoch 266/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5740 - euclidean_distance_loss: 1.0455 - val_loss: 1.8368 - val_euclidean_distance_loss: 1.3083\n",
      "Epoch 267/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5791 - euclidean_distance_loss: 1.0507 - val_loss: 1.8558 - val_euclidean_distance_loss: 1.3274\n",
      "Epoch 268/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6004 - euclidean_distance_loss: 1.0721 - val_loss: 1.8743 - val_euclidean_distance_loss: 1.3460\n",
      "Epoch 269/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6548 - euclidean_distance_loss: 1.1264 - val_loss: 1.8304 - val_euclidean_distance_loss: 1.3022\n",
      "Epoch 270/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6497 - euclidean_distance_loss: 1.1216 - val_loss: 1.8718 - val_euclidean_distance_loss: 1.3437\n",
      "Epoch 271/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6197 - euclidean_distance_loss: 1.0916 - val_loss: 1.9550 - val_euclidean_distance_loss: 1.4270\n",
      "Epoch 272/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6707 - euclidean_distance_loss: 1.1427 - val_loss: 1.8601 - val_euclidean_distance_loss: 1.3322\n",
      "Epoch 273/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6093 - euclidean_distance_loss: 1.0814 - val_loss: 1.8549 - val_euclidean_distance_loss: 1.3270\n",
      "Epoch 274/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5964 - euclidean_distance_loss: 1.0685 - val_loss: 1.8994 - val_euclidean_distance_loss: 1.3717\n",
      "Epoch 275/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5981 - euclidean_distance_loss: 1.0702 - val_loss: 1.8797 - val_euclidean_distance_loss: 1.3520\n",
      "Epoch 276/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5876 - euclidean_distance_loss: 1.0600 - val_loss: 1.8642 - val_euclidean_distance_loss: 1.3367\n",
      "Epoch 277/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6025 - euclidean_distance_loss: 1.0750 - val_loss: 1.8947 - val_euclidean_distance_loss: 1.3675\n",
      "Epoch 278/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5738 - euclidean_distance_loss: 1.0465 - val_loss: 1.8425 - val_euclidean_distance_loss: 1.3152\n",
      "Epoch 279/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.5719 - euclidean_distance_loss: 1.0447 - val_loss: 1.8428 - val_euclidean_distance_loss: 1.3156\n",
      "Epoch 280/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.5707 - euclidean_distance_loss: 1.0436 - val_loss: 1.8395 - val_euclidean_distance_loss: 1.3124\n",
      "Epoch 281/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5621 - euclidean_distance_loss: 1.0350 - val_loss: 1.8272 - val_euclidean_distance_loss: 1.3003\n",
      "Epoch 282/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5608 - euclidean_distance_loss: 1.0338 - val_loss: 1.8758 - val_euclidean_distance_loss: 1.3489\n",
      "Epoch 283/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.5793 - euclidean_distance_loss: 1.0525 - val_loss: 1.8689 - val_euclidean_distance_loss: 1.3421\n",
      "Epoch 284/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5556 - euclidean_distance_loss: 1.0288 - val_loss: 1.8501 - val_euclidean_distance_loss: 1.3234\n",
      "Epoch 285/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5493 - euclidean_distance_loss: 1.0226 - val_loss: 1.8125 - val_euclidean_distance_loss: 1.2859\n",
      "Epoch 286/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5403 - euclidean_distance_loss: 1.0136 - val_loss: 1.7988 - val_euclidean_distance_loss: 1.2723\n",
      "Epoch 287/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.5239 - euclidean_distance_loss: 0.9974 - val_loss: 1.8065 - val_euclidean_distance_loss: 1.2801\n",
      "Epoch 288/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5214 - euclidean_distance_loss: 0.9950 - val_loss: 1.8039 - val_euclidean_distance_loss: 1.2776\n",
      "Epoch 289/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5304 - euclidean_distance_loss: 1.0041 - val_loss: 1.8122 - val_euclidean_distance_loss: 1.2860\n",
      "Epoch 290/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5449 - euclidean_distance_loss: 1.0187 - val_loss: 1.8275 - val_euclidean_distance_loss: 1.3013\n",
      "Epoch 291/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5547 - euclidean_distance_loss: 1.0286 - val_loss: 1.8105 - val_euclidean_distance_loss: 1.2843\n",
      "Epoch 292/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5593 - euclidean_distance_loss: 1.0332 - val_loss: 1.8806 - val_euclidean_distance_loss: 1.3546\n",
      "Epoch 293/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5549 - euclidean_distance_loss: 1.0290 - val_loss: 1.8491 - val_euclidean_distance_loss: 1.3232\n",
      "Epoch 294/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.5337 - euclidean_distance_loss: 1.0079 - val_loss: 1.7936 - val_euclidean_distance_loss: 1.2679\n",
      "Epoch 295/310\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.5177 - euclidean_distance_loss: 0.9920 - val_loss: 1.8108 - val_euclidean_distance_loss: 1.2853\n",
      "Epoch 296/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5253 - euclidean_distance_loss: 0.9997 - val_loss: 1.8524 - val_euclidean_distance_loss: 1.3269\n",
      "Epoch 297/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5390 - euclidean_distance_loss: 1.0135 - val_loss: 1.7947 - val_euclidean_distance_loss: 1.2693\n",
      "Epoch 298/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5673 - euclidean_distance_loss: 1.0418 - val_loss: 1.8132 - val_euclidean_distance_loss: 1.2879\n",
      "Epoch 299/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5499 - euclidean_distance_loss: 1.0246 - val_loss: 1.7901 - val_euclidean_distance_loss: 1.2649\n",
      "Epoch 300/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.5142 - euclidean_distance_loss: 0.9890 - val_loss: 1.8263 - val_euclidean_distance_loss: 1.3011\n",
      "Epoch 301/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5198 - euclidean_distance_loss: 0.9946 - val_loss: 1.7834 - val_euclidean_distance_loss: 1.2582\n",
      "Epoch 302/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5053 - euclidean_distance_loss: 0.9803 - val_loss: 1.7955 - val_euclidean_distance_loss: 1.2706\n",
      "Epoch 303/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.5166 - euclidean_distance_loss: 0.9917 - val_loss: 1.7745 - val_euclidean_distance_loss: 1.2497\n",
      "Epoch 304/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.4973 - euclidean_distance_loss: 0.9725 - val_loss: 1.7807 - val_euclidean_distance_loss: 1.2557\n",
      "Epoch 305/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5148 - euclidean_distance_loss: 0.9901 - val_loss: 1.7749 - val_euclidean_distance_loss: 1.2504\n",
      "Epoch 306/310\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.5159 - euclidean_distance_loss: 0.9913 - val_loss: 1.8125 - val_euclidean_distance_loss: 1.2879\n",
      "Epoch 307/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5107 - euclidean_distance_loss: 0.9861 - val_loss: 1.8110 - val_euclidean_distance_loss: 1.2865\n",
      "Epoch 308/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5126 - euclidean_distance_loss: 0.9881 - val_loss: 1.8003 - val_euclidean_distance_loss: 1.2760\n",
      "Epoch 309/310\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5124 - euclidean_distance_loss: 0.9881 - val_loss: 1.7746 - val_euclidean_distance_loss: 1.2505\n",
      "Epoch 310/310\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5216 - euclidean_distance_loss: 0.9974 - val_loss: 1.7949 - val_euclidean_distance_loss: 1.2707\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "29/29 [==============================] - 0s 875us/step\n",
      "TR Loss:  1.5215617418289185\n",
      "VL Loss:  1.794877290725708\n",
      "TS Loss:  1.3299745601740574\n",
      "Training Time: 11.30 seconds\n",
      "keras end\n",
      "[[  9.620195     8.804251    19.775124  ]\n",
      " [  8.766124   -38.08366     34.793484  ]\n",
      " [  9.888325     5.1349316   19.96482   ]\n",
      " ...\n",
      " [  5.6530204    9.360663     0.25347847]\n",
      " [  4.0433846  -69.716934    28.362469  ]\n",
      " [-69.56984      6.2394934   11.950552  ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keras_nn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
